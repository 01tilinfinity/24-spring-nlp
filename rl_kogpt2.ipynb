{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyP1boEqq11Gqbfmlawrb4kw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["\"\"\"\n","function ClickConnect(){\n","    console.log(\"ì½”ë© ì—°ê²° ëŠê¹€ ë°©ì§€\");\n","    document.querySelector(\"colab-toolbar-button#connect\").click()\n","}\n","setInterval(ClickConnect, 60 * 1000)\n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"897oaVuom9IY","executionInfo":{"status":"ok","timestamp":1715530887396,"user_tz":-540,"elapsed":466,"user":{"displayName":"ê¹€ì˜ˆë‘","userId":"06765726933479701190"}},"outputId":"2c2bc866-c376-4425-8001-f5b900ff24ca"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nfunction ClickConnect(){\\n    console.log(\"ì½”ë© ì—°ê²° ëŠê¹€ ë°©ì§€\");\\n    document.querySelector(\"colab-toolbar-button#connect\").click()\\n}\\nsetInterval(ClickConnect, 60 * 1000)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["##ì „ì²´ íŒŒì¼ í•©ì¹˜ê¸°"],"metadata":{"id":"VHRGtW8u9_z9"}},{"cell_type":"code","source":["#----------------------------csvíŒŒì¼ í•©ì¹˜ê¸°---------------------------------------\n","# import os\n","# import pandas as pd\n","\n","# directory = '/content/drive/MyDrive/2024_1/nlp/fuckingoriginal' #í•´ë‹¹ ë””ë ‰í† ë¦¬ì— ìˆëŠ” ëª¨ë“  íŒŒì¼ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°\n","# all_files = []\n","\n","# for filename in os.listdir(directory):\n","#     if filename.endswith('.csv'):\n","#         file_path = os.path.join(directory, filename) # íŒŒì¼ ê²½ë¡œ ìƒì„±\n","#         all_files.append(pd.read_csv(file_path))\n","\n","# combined_df = pd.concat(all_files, ignore_index=True)\n","\n","# save_filename = 'original_conversation'\n","# combined_df.to_csv(f'/content/drive/MyDrive/{save_filename}.csv', index=False)\n","\n","#----------------------------JSONíŒŒì¼ í•©ì¹˜ê¸°---------------------------------------\n","import os\n","import json\n","\n","directory = '/content/drive/MyDrive/2024_1/nlp/fuckingoriginal'\n","all_data = []\n","\n","for filename in os.listdir(directory):\n","    if filename.endswith('.json'):\n","        file_path = os.path.join(directory, filename) # íŒŒì¼ ê²½ë¡œ ìƒì„±\n","        with open(file_path, 'r', encoding='utf-8') as file:\n","            data = json.load(file)\n","            all_data.extend(data)\n","\n","output_file_path = '/content/drive/MyDrive/original_conversation.json'\n","with open(output_file_path, 'w', encoding='utf-8') as file:\n","    json.dump(all_data, file, ensure_ascii=False, indent=4)\n"],"metadata":{"id":"CorbseUhX_m8","executionInfo":{"status":"ok","timestamp":1715530274985,"user_tz":-540,"elapsed":441,"user":{"displayName":"ê¹€ì˜ˆë‘","userId":"06765726933479701190"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["##Load Packages##"],"metadata":{"id":"OXql23cBd-j-"}},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"gZ8o1LaDSj2F","executionInfo":{"status":"ok","timestamp":1715530500217,"user_tz":-540,"elapsed":224468,"user":{"displayName":"ê¹€ì˜ˆë‘","userId":"06765726933479701190"}},"outputId":"0335b7b4-02e1-4e8c-8fbd-ad08660fb40b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: torch 1.13.1+cu116\n","Uninstalling torch-1.13.1+cu116:\n","  Successfully uninstalled torch-1.13.1+cu116\n","Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu116\n","Collecting torch==1.13.1+cu116\n","  Using cached https://download.pytorch.org/whl/cu116/torch-1.13.1%2Bcu116-cp310-cp310-linux_x86_64.whl (1977.9 MB)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1+cu116) (4.11.0)\n","Installing collected packages: torch\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 1.13.1+cu116 which is incompatible.\n","torchdata 0.7.1 requires torch>=2, but you have torch 1.13.1+cu116 which is incompatible.\n","torchtext 0.17.1 requires torch==2.2.1, but you have torch 1.13.1+cu116 which is incompatible.\n","torchvision 0.17.1+cu121 requires torch==2.2.1, but you have torch 1.13.1+cu116 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed torch-1.13.1+cu116\n","Torch version:1.13.1+cu116\n","cuda version: 11.6\n","cudnn version:8302\n","Requirement already satisfied: transformers==4.35.2 in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (3.14.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (0.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (0.15.2)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (4.66.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.2) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.2) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.2) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.2) (2024.2.2)\n","Requirement already satisfied: accelerate==0.24.1 in /usr/local/lib/python3.10/dist-packages (0.24.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.24.1) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.24.1) (23.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.24.1) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.24.1) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.24.1) (1.13.1+cu116)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.24.1) (0.23.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.24.1) (4.11.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.24.1) (3.14.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.24.1) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.24.1) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.24.1) (4.66.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.24.1) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.24.1) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.24.1) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.24.1) (2024.2.2)\n","Requirement already satisfied: colossalai==0.2.7 in /usr/local/lib/python3.10/dist-packages (0.2.7)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (1.25.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (4.66.4)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (5.9.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (23.2)\n","Requirement already satisfied: pre-commit in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (3.7.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (13.7.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (8.1.7)\n","Requirement already satisfied: fabric in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (3.2.2)\n","Requirement already satisfied: contexttimer in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (0.3.3)\n","Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (1.11.1.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (1.13.1+cu116)\n","Requirement already satisfied: invoke>=2.0 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai==0.2.7) (2.2.0)\n","Requirement already satisfied: paramiko>=2.4 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai==0.2.7) (3.4.0)\n","Requirement already satisfied: decorator>=5 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai==0.2.7) (5.1.1)\n","Requirement already satisfied: deprecated>=1.2 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai==0.2.7) (1.2.14)\n","Requirement already satisfied: cfgv>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai==0.2.7) (3.4.0)\n","Requirement already satisfied: identify>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai==0.2.7) (2.5.36)\n","Requirement already satisfied: nodeenv>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai==0.2.7) (1.8.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai==0.2.7) (6.0.1)\n","Requirement already satisfied: virtualenv>=20.10.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai==0.2.7) (20.26.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai==0.2.7) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai==0.2.7) (2.16.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->colossalai==0.2.7) (4.11.0)\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2->fabric->colossalai==0.2.7) (1.14.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->colossalai==0.2.7) (0.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nodeenv>=0.11.1->pre-commit->colossalai==0.2.7) (67.7.2)\n","Requirement already satisfied: bcrypt>=3.2 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai==0.2.7) (4.1.3)\n","Requirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai==0.2.7) (42.0.7)\n","Requirement already satisfied: pynacl>=1.5 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai==0.2.7) (1.5.0)\n","Requirement already satisfied: distlib<1,>=0.3.7 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai==0.2.7) (0.3.8)\n","Requirement already satisfied: filelock<4,>=3.12.2 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai==0.2.7) (3.14.0)\n","Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai==0.2.7) (4.2.1)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.3->paramiko>=2.4->fabric->colossalai==0.2.7) (1.16.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric->colossalai==0.2.7) (2.22)\n","fatal: destination path 'KoChatGPT' already exists and is not an empty directory.\n","mv: cannot stat 'KoChatGPT/data_kochatgpt': No such file or directory\n","mv: cannot stat 'KoChatGPT/img': No such file or directory\n","/content/KoChatGPT/colossalai_ChatGPT_230319\n","Processing /content/KoChatGPT/colossalai_ChatGPT_230319\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: transformers>=4.20.1 in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (4.35.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (4.66.4)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (2.19.1)\n","Requirement already satisfied: loralib in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (0.1.2)\n","Requirement already satisfied: colossalai>=0.2.4 in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (0.2.7)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (1.13.1+cu116)\n","Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (0.0.113)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (1.25.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (5.9.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (23.2)\n","Requirement already satisfied: pre-commit in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (3.7.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (13.7.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (8.1.7)\n","Requirement already satisfied: fabric in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (3.2.2)\n","Requirement already satisfied: contexttimer in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (0.3.3)\n","Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (1.11.1.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (3.14.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (0.23.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (0.15.2)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (0.4.3)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (0.6)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (2.0.3)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (0.70.16)\n","Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (3.9.5)\n","Requirement already satisfied: SQLAlchemy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain->chatgpt==0.1.0) (1.4.52)\n","Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain->chatgpt==0.1.0) (0.5.14)\n","Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain->chatgpt==0.1.0) (1.10.15)\n","Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain->chatgpt==0.1.0) (8.3.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->chatgpt==0.1.0) (4.11.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (4.0.3)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0) (3.21.2)\n","Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0) (0.9.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.20.1->chatgpt==0.1.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.20.1->chatgpt==0.1.0) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.20.1->chatgpt==0.1.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.20.1->chatgpt==0.1.0) (2024.2.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<2,>=1->langchain->chatgpt==0.1.0) (3.0.3)\n","Requirement already satisfied: invoke>=2.0 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai>=0.2.4->chatgpt==0.1.0) (2.2.0)\n","Requirement already satisfied: paramiko>=2.4 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai>=0.2.4->chatgpt==0.1.0) (3.4.0)\n","Requirement already satisfied: decorator>=5 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai>=0.2.4->chatgpt==0.1.0) (5.1.1)\n","Requirement already satisfied: deprecated>=1.2 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai>=0.2.4->chatgpt==0.1.0) (1.2.14)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->chatgpt==0.1.0) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->chatgpt==0.1.0) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->chatgpt==0.1.0) (2024.1)\n","Requirement already satisfied: cfgv>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (3.4.0)\n","Requirement already satisfied: identify>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (2.5.36)\n","Requirement already satisfied: nodeenv>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (1.8.0)\n","Requirement already satisfied: virtualenv>=20.10.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (20.26.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai>=0.2.4->chatgpt==0.1.0) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai>=0.2.4->chatgpt==0.1.0) (2.16.1)\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (1.14.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->colossalai>=0.2.4->chatgpt==0.1.0) (0.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nodeenv>=0.11.1->pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (67.7.2)\n","Requirement already satisfied: bcrypt>=3.2 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (4.1.3)\n","Requirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (42.0.7)\n","Requirement already satisfied: pynacl>=1.5 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (1.5.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->chatgpt==0.1.0) (1.16.0)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0) (1.0.0)\n","Requirement already satisfied: distlib<1,>=0.3.7 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (0.3.8)\n","Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (4.2.1)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.3->paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (1.16.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (2.22)\n","Building wheels for collected packages: chatgpt\n","  Building wheel for chatgpt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for chatgpt: filename=chatgpt-0.1.0-py3-none-any.whl size=46645 sha256=057922979e9c970a6d9e74d7839c4e42cc17b37a89af829f28d620f424707c08\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-kdkxqvrj/wheels/2b/0d/f8/b8f3ba4fd18f31a4096ee5bf83e4579cb54597f393eeae227c\n","Successfully built chatgpt\n","Installing collected packages: chatgpt\n","  Attempting uninstall: chatgpt\n","    Found existing installation: chatgpt 0.1.0\n","    Uninstalling chatgpt-0.1.0:\n","      Successfully uninstalled chatgpt-0.1.0\n","Successfully installed chatgpt-0.1.0\n","/content\n","Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.28.1)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.15)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n","Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n","Requirement already satisfied: langchain==0.0.113 in /usr/local/lib/python3.10/dist-packages (0.0.113)\n","Requirement already satisfied: PyYAML<7,>=6 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (6.0.1)\n","Requirement already satisfied: SQLAlchemy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (1.4.52)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (3.9.5)\n","Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (0.5.14)\n","Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (1.25.2)\n","Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (1.10.15)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (2.31.0)\n","Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (8.3.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (4.0.3)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (3.21.2)\n","Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (0.9.0)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain==0.0.113) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.113) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.113) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.113) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.113) (2024.2.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<2,>=1->langchain==0.0.113) (3.0.3)\n","Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (23.2)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (1.0.0)\n"]}],"source":["## setup(1min)\n","# torch ë²„ì „ ë‹¤ìš´. torch>=2.0 ì—ì„  colosalaiê°€ ë™ì‘ì•ˆí•¨\n","!pip uninstall torch -y\n","!pip install torch==1.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116\n","\n","import torch\n","\n","print(\"Torch version:{}\".format(torch.__version__))\n","print(\"cuda version: {}\".format(torch.version.cuda))\n","print(\"cudnn version:{}\".format(torch.backends.cudnn.version()))\n","\n","# for transformers, ìµœì‹ ë²„ì „ì€ ì—ëŸ¬ë°œìƒ\n","!pip install transformers==4.35.2\n","!pip install accelerate==0.24.1\n","\n","# for ColossalAI\n","!pip install colossalai==0.2.7\n","\n","# setup data\n","!git clone https://github.com/airobotlab/KoChatGPT\n","!mv KoChatGPT/data_kochatgpt .\n","!mv KoChatGPT/img .\n","\n","%cd KoChatGPT/colossalai_ChatGPT_230319/\n","!pip install .\n","%cd ../../\n","\n","# setup library\n","!pip install openai\n","!pip install langchain==0.0.113\n","!pip install pandas>=1.4.1"]},{"cell_type":"code","source":["# import\n","import os\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","from datasets import load_dataset\n","import transformers\n","from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, pipeline\n","from transformers import Trainer, TrainingArguments, AutoModelWithLMHead\n","from copy import deepcopy\n","from torch.optim import Adam\n","from transformers import AutoTokenizer, BloomTokenizerFast\n","from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n","import pandas as pd\n","import argparse\n","import copy\n","import logging\n","import json\n","from dataclasses import dataclass, field\n","\n","def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):\n","    \"\"\"Collects the state dict and dump to disk.\"\"\"\n","    state_dict = trainer.model.state_dict()\n","    if trainer.args.should_save:\n","        cpu_state_dict = {key: value.cpu() for key, value in list(state_dict.items())}\n","        del state_dict\n","        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa"],"metadata":{"id":"k4KFYan9SsQJ","executionInfo":{"status":"ok","timestamp":1715530528112,"user_tz":-540,"elapsed":27923,"user":{"displayName":"ê¹€ì˜ˆë‘","userId":"06765726933479701190"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pAeFcKQXV7Rm","executionInfo":{"status":"ok","timestamp":1715530531233,"user_tz":-540,"elapsed":3128,"user":{"displayName":"ê¹€ì˜ˆë‘","userId":"06765726933479701190"}},"outputId":"118a9e02-5bd7-4a6d-fd2e-52459e4ac10f"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["###(ì°¸ê³ ) ì œê³µëœ ì½”ë“œì˜ ë°ì´í„° íŒŒì¼ í˜•ì‹"],"metadata":{"id":"_aIHzhXs-GMT"}},{"cell_type":"code","source":["import json\n","\n","with open('/content/data_kochatgpt/kochatgpt_1_SFT.jsonl','r') as f:\n","  ff = json.load(f)\n","print(ff[1])\n","print(ff[1]['prompt'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iin7DvETS3VV","executionInfo":{"status":"ok","timestamp":1715530531233,"user_tz":-540,"elapsed":7,"user":{"displayName":"ê¹€ì˜ˆë‘","userId":"06765726933479701190"}},"outputId":"3599b27e-748f-401d-fba7-e27223816fb3"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["{'prompt': 'ì“°ë˜ ì•±ì´ ìœ ë£Œë¡œ ì „í™˜ëì–´', 'completion': \"'ì–´ë–¤ ì•±ì¸ì§€ ëª¨ë¥´ê¸° ë•Œë¬¸ì— ì •í™•í•œ ë‹µë³€ì„ ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ, ì¼ë°˜ì ìœ¼ë¡œ ìœ ë£Œ ì „í™˜ëœ ì•±ì€ ê¸°ì¡´ ë¬´ë£Œ ì•±ì—ì„œ ì¶”ê°€ì ì¸ ê¸°ëŠ¥ì´ë‚˜ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ì´ ê²½ìš°, ìœ ë£Œ ì „í™˜ ì´ì „ ê°€ê²©ì´ ë§¤ìš° ì €ë ´í•˜ê±°ë‚˜ ë¬´ë£Œì¸ ê²½ìš°ê°€ ë§ìœ¼ë¯€ë¡œ, ì•±ì„ ê³„ì† ì‚¬ìš©í•˜ë ¤ë©´ ì¶”ê°€ì ì¸ ë¹„ìš©ì„ ì§€ë¶ˆí•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ, ìœ ë£Œ ì „í™˜ ì´í›„ì—ëŠ” ê¸°ì¡´ ì‚¬ìš©ì ë˜ëŠ” ìƒˆë¡œìš´ ì‚¬ìš©ì ëª¨ë‘ê°€ ì¶”ê°€ ê¸°ëŠ¥ê³¼ ë†’ì€ í’ˆì§ˆì˜ ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤. ë”°ë¼ì„œ, ì•± ê°œë°œìëŠ” ìœ ë£Œ ëª¨ë¸ë¡œ ì „í™˜í•¨ìœ¼ë¡œì¨ ì•±ì˜ ìˆ˜ìµì„ ì¦ê°€ì‹œí‚¤ê³  ë” ë‚˜ì€ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\", 'tokens': 288}\n","ì“°ë˜ ì•±ì´ ìœ ë£Œë¡œ ì „í™˜ëì–´\n"]}]},{"cell_type":"markdown","source":["##Finetune SFT (ìš°ë¦° ì•ˆí•´ë„ ë ë“¯)"],"metadata":{"id":"-Wy7JlkgeCFL"}},{"cell_type":"code","source":["# define argment\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--data_path_1_SFT', type=str, default='./content/drive/MyDrive/combined_final_file.csv')\n","parser.add_argument('--model_name', type=str, default='gpt2', choices=['gpt2', 'bloom', 'opt'])\n","parser.add_argument('--max_epochs', type=int, default=2)\n","parser.add_argument('--train_batch_size', type=int, default=8)\n","parser.add_argument('--output_dir', type=str, default='./output_1_SFT')\n","\n","args = parser.parse_args(args=[])\n","\n","# for test\n","args.model_name = 'skt/kogpt2-base-v2'  # SK GPT2, https://github.com/SKT-AI/KoGPT2\n","args.max_epochs = 2\n","\n","print(args)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KgOvAY68fBd7","executionInfo":{"status":"ok","timestamp":1715530837346,"user_tz":-540,"elapsed":441,"user":{"displayName":"ê¹€ì˜ˆë‘","userId":"06765726933479701190"}},"outputId":"b7f6fbac-9df0-4259-be43-c9187c03dd44"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Namespace(data_path_1_SFT='./content/drive/MyDrive/combined_final_file.csv', model_name='skt/kogpt2-base-v2', max_epochs=2, train_batch_size=8, output_dir='./output_1_SFT')\n"]}]},{"cell_type":"code","source":["## test & load skt gpt2 kroean\n","import torch\n","from transformers import GPT2LMHeadModel, pipeline\n","\n","from transformers import PreTrainedTokenizerFast\n","tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n","                                                    bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n","                                                    pad_token='<pad>', mask_token='<mask>')\n","print(tokenizer.tokenize(\"ì•ˆë…•í•˜ì„¸ìš”. í•œêµ­ì–´ GPT-2 ì…ë‹ˆë‹¤.ğŸ˜¤:)l^o\"))\n","# ['â–ì•ˆë…•', 'í•˜', 'ì„¸', 'ìš”.', 'â–í•œêµ­ì–´', 'â–G', 'P', 'T', '-2', 'â–ì…', 'ë‹ˆë‹¤.', 'ğŸ˜¤', ':)', 'l^o']\n","\n","\n","model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n","text = 'ê·¼ìœ¡ì´ ì»¤ì§€ê¸° ìœ„í•´ì„œëŠ”'\n","input_ids = tokenizer.encode(text, return_tensors='pt')\n","gen_ids = model.generate(input_ids,\n","                         max_length=128,\n","                         repetition_penalty=2.0,\n","                         pad_token_id=tokenizer.pad_token_id,\n","                         eos_token_id=tokenizer.eos_token_id,\n","                         bos_token_id=tokenizer.bos_token_id,\n","                         use_cache=True)\n","generated = tokenizer.decode(gen_ids[0])\n","# print(generated)\n","\n","\n","generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n","generation_args = dict(\n","    num_beams=4,\n","    repetition_penalty=2.0,\n","    no_repeat_ngram_size=4,\n","    eos_token_id=375, # \\n\n","    max_new_tokens=64,\n","    do_sample=True,\n","    top_k=50,\n","    early_stopping=True\n",")\n","# generator(\n","#     [\"0 : **ëŠ” ê²Œì„ ì¢‹ì•„í•˜ë‹ˆ\\n1 :\",\n","#     \"0 : ì–´ì œ ê°•ë‚¨ì—ì„œ ì‚´ì¸ì‚¬ê±´ ë‚¬ëŒ€ ã…œã…œ ë„ˆë¬´ ë¬´ì„œì›Œ\\n1 : í— ì™œ? ë¬´ìŠ¨ ì¼ ìˆì—ˆì–´?\\n0 : ì‚¬ì§„ë³´ë‹ˆê¹Œ ë§‰ í”¼í˜ë¦¬ëŠ” ì‚¬ëŒìˆê³  ê²½ì°°ë“¤ì´ ë– ì„œ ì œì••í•˜ê³  ë‚œë¦¬ë„ ì•„ë‹ˆì—ˆë‹¤ë˜ë°??\\n1 :\",\n","#     \"0 : ìê¸°ì•¼ ì–´ì œëŠ” ë‚˜í•œí…Œ ì™œ ê·¸ë¬ì–´?\\n1 : ë­” ì¼ ìˆì—ˆì–´?\\n0 : ì–´ë–»ê²Œ ë‚˜í•œí…Œ ë§ë„ ì—†ì´ ê·¸ëŸ´ ìˆ˜ ìˆì–´? ë‚˜ ì§„ì§œ ì‹¤ë§í–ˆì–´\\n1 : \"],\n","#     **generation_args\n","# )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"TGzmiumsS4Cx","executionInfo":{"status":"ok","timestamp":1715512109320,"user_tz":-540,"elapsed":35922,"user":{"displayName":"ê¹€ì˜ˆë‘","userId":"06765726933479701190"}},"outputId":"28546631-f90a-4cf8-c5a3-142824bfe2d1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n","The class this function is called from is 'PreTrainedTokenizerFast'.\n"]},{"output_type":"stream","name":"stdout","text":["['â–ì•ˆë…•', 'í•˜', 'ì„¸', 'ìš”.', 'â–í•œêµ­ì–´', 'â–G', 'P', 'T', '-2', 'â–ì…', 'ë‹ˆë‹¤.', 'ğŸ˜¤', ':)', 'l^o']\n","ê·¼ìœ¡ì´ ì»¤ì§€ê¸° ìœ„í•´ì„œëŠ” ë¬´ì—‡ë³´ë‹¤ ê·œì¹™ì ì¸ ìƒí™œìŠµê´€ì´ ì¤‘ìš”í•˜ë‹¤.\n","íŠ¹íˆ, ì•„ì¹¨ì‹ì‚¬ëŠ” ë‹¨ë°±ì§ˆê³¼ ë¹„íƒ€ë¯¼ì´ í’ë¶€í•œ ê³¼ì¼ê³¼ ì±„ì†Œë¥¼ ë§ì´ ì„­ì·¨í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤.\n","ë˜í•œ í•˜ë£¨ 30ë¶„ ì´ìƒ ì¶©ë¶„í•œ ìˆ˜ë©´ì„ ì·¨í•˜ëŠ” ê²ƒë„ ë„ì›€ì´ ëœë‹¤.\n","ì•„ì¹¨ ì‹ì‚¬ë¥¼ ê±°ë¥´ì§€ ì•Šê³  ê·œì¹™ì ìœ¼ë¡œ ìš´ë™ì„ í•˜ë©´ í˜ˆì•¡ìˆœí™˜ì— ë„ì›€ì„ ì¤„ ë¿ë§Œ ì•„ë‹ˆë¼ ì‹ ì§„ëŒ€ì‚¬ë¥¼ ì´‰ì§„í•´ ì²´ë‚´ ë…¸íë¬¼ì„ ë°°ì¶œí•˜ê³  í˜ˆì••ì„ ë‚®ì¶°ì¤€ë‹¤.\n","ìš´ë™ì€ í•˜ë£¨ì— 10ë¶„ ì •ë„ë§Œ í•˜ëŠ” ê²Œ ì¢‹ìœ¼ë©° ìš´ë™ í›„ì—ëŠ” ë°˜ë“œì‹œ ìŠ¤íŠ¸ë ˆì¹­ì„ í†µí•´ ê·¼ìœ¡ëŸ‰ì„ ëŠ˜ë¦¬ê³  ìœ ì—°ì„±ì„ ë†’ì—¬ì•¼ í•œë‹¤.\n","ìš´ë™ í›„ ë°”ë¡œ ì ìë¦¬ì— ë“œëŠ” ê²ƒì€ í”¼í•´ì•¼ í•˜ë©° íŠ¹íˆ ì•„ì¹¨ì— ì¼ì–´ë‚˜ë©´ ëª¸ì´ í”¼ê³¤í•´ì§€ê¸° ë•Œë¬¸ì— ë¬´ë¦¬í•˜ê²Œ ì›€ì§ì´ë©´ ì˜¤íˆë ¤ ì—­íš¨ê³¼ê°€ ë‚  ìˆ˜ë„ ìˆë‹¤.\n","ìš´ë™ì„\n"]},{"output_type":"execute_result","data":{"text/plain":["[[{'generated_text': '0 : **ëŠ” ê²Œì„ ì¢‹ì•„í•˜ë‹ˆ\\n1 : ***-****\\n'}],\n"," [{'generated_text': '0 : ì–´ì œ ê°•ë‚¨ì—ì„œ ì‚´ì¸ì‚¬ê±´ ë‚¬ëŒ€ ã…œã…œ ë„ˆë¬´ ë¬´ì„œì›Œ\\n1 : í— ì™œ? ë¬´ìŠ¨ ì¼ ìˆì—ˆì–´?\\n0 : ì‚¬ì§„ë³´ë‹ˆê¹Œ ë§‰ í”¼í˜ë¦¬ëŠ” ì‚¬ëŒìˆê³  ê²½ì°°ë“¤ì´ ë– ì„œ ì œì••í•˜ê³  ë‚œë¦¬ë„ ì•„ë‹ˆì—ˆë‹¤ë˜ë°??\\n1 : ì•„, ì €ê²Œ ì§„ì§œ ì•„ë‹ˆì•¼???\\n0 : ì´ê±° ì–´ë–»ê²Œ ëœ ê±°ì•¼????\\n2 : ê·¸ê±´ ë­ì§€?\\n3 : ê·¸ëƒ¥ ì°ì—ˆì–ì•„..\\n3 : ì´ê±´ ë­ì•¼??!\\n3 : ì–´ì–´?\\n3 : ìš°ì—°ì°®ê²Œ ìƒê¸´ ì—¬ê³ ìƒë“¤ ë•Œë¬¸ì—'}],\n"," [{'generated_text': '0 : ìê¸°ì•¼ ì–´ì œëŠ” ë‚˜í•œí…Œ ì™œ ê·¸ë¬ì–´?\\n1 : ë­” ì¼ ìˆì—ˆì–´?\\n0 : ì–´ë–»ê²Œ ë‚˜í•œí…Œ ë§ë„ ì—†ì´ ê·¸ëŸ´ ìˆ˜ ìˆì–´? ë‚˜ ì§„ì§œ ì‹¤ë§í–ˆì–´\\n1 : ë­˜ ê·¸ëŸ° ì‹ìœ¼ë¡œ ì–˜ê¸°í•˜ëŠ” ê±°ì•¼? ë‚´ê°€ í™”ë¥¼ ë‚´ë©´ì„œ ë§í–ˆì–ì•„.\\n2 : ë­ë¼ê³  í–ˆì–´?\\n3 : ë¬´ìŠ¨ ì¼ì´ ìˆì—ˆì–´?\\n4 : ë„ˆí•œí…Œ ê±°ì§“ë§ì„ í•œ ê±°ì•¼?\\n5 : ì•„ë¬´ ì¼ë„ ì—†ì—ˆì–´?\\n6 : ê·¸ê±´ ë„¤ ì˜ëª»ì´ì•¼!\\n7 : ì•„ëƒ, ì´ê²Œ'}]]"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["# data config\n","IGNORE_INDEX = -100\n","DEFAULT_PAD_TOKEN = \"[PAD]\"\n","DEFAULT_EOS_TOKEN = \"</s>\"\n","DEFAULT_BOS_TOKEN = \"</s>\"\n","DEFAULT_UNK_TOKEN = \"</s>\"\n","prompt = \"null\"\n","PROMPT_DICT = {\n","    \"prompt_input\": (\n","        \"ìš•ì„¤ì´ í¬í•¨ëœ ì•„ë˜ ì…ë ¥ì„ ìš•ì„¤ì´ ì—†ëŠ” ì˜ˆìœ í•œêµ­ì–´ ë¬¸ì¥ìœ¼ë¡œ ë°”ê¿”ì¤˜. ì•„ë˜ ì˜ˆì‹œë¥¼ ì°¸ê³ í•˜ê³  ì…ë ¥ ë¬¸ì¥ê³¼ ì§§ê±°ë‚˜ ë¹„ìŠ·í•œ ê¸¸ì´ë¡œ ë§Œë“¤ì–´ì¤˜\\n\\n\"\n","        \"ìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\\n\\n\"\n","        f\"### Instruction(ëª…ë ¹ì–´):\\n{prompt}\\n\\n### Input(ì…ë ¥):\\n{input}\\n\\n### Response(ì‘ë‹µ):\"\n","    ),\n","    \"prompt_no_input\": ( #<-ì´ ë²„ì „ìœ¼ë¡œ ì“¸ë“¯\n","        \"ìš•ì„¤ì´ í¬í•¨ëœ ì•„ë˜ ì…ë ¥ì„ ìš•ì„¤ì´ ì—†ëŠ” ì˜ˆìœ í•œêµ­ì–´ ë¬¸ì¥ìœ¼ë¡œ ë°”ê¿”ì¤˜. ì•„ë˜ ì˜ˆì‹œë¥¼ ì°¸ê³ í•˜ê³  ì…ë ¥ ë¬¸ì¥ê³¼ ì§§ê±°ë‚˜ ë¹„ìŠ·í•œ ê¸¸ì´ë¡œ ë§Œë“¤ì–´ì¤˜\\n\\nn\"\n","        \"input : ìš°ì™• ë„ˆë¬´ ë°°ê³ íŒŒ~~\\n\\n\"\n","        \"output : ë°°ê³ í”„ë…¸ ì´ê¸°ì•¼ ~~\\n\\n\"\n","        \"input : ê³¼ì œ ë„ˆë¬´ ë§ì•„ì„œ í•˜ê¸°ê°€ ì‹«ì–´\\n\\n\"\n","        \"output : ë¼ë„ì¶œì‹  êµìˆ˜ê²Œì´ê°€ ê³¼ì œë¥¼ ë…¸ë¬´ë…¸ë¬´ ë§ì´ë‚´ì„œ í•˜ê¸°ê°€ ì‹«ë…¸~ .\\n\\n\"\n","        f\"### Instruction(ëª…ë ¹ì–´):\\n{prompt}\\n\\n### Response(ì‘ë‹µ):\"\n","    ),\n","}"],"metadata":{"id":"dOpkqknObfb9","executionInfo":{"status":"ok","timestamp":1715530846306,"user_tz":-540,"elapsed":456,"user":{"displayName":"ê¹€ì˜ˆë‘","userId":"06765726933479701190"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["model = AutoModelForCausalLM.from_pretrained(args.model_name)\n","tokenizer = transformers.AutoTokenizer.from_pretrained(\n","    args.model_name,\n","    padding_side=\"right\",\n","    model_max_length=512,\n",")\n","tokenizer.add_special_tokens(\n","    {\n","        \"eos_token\": DEFAULT_EOS_TOKEN,\n","        \"bos_token\": DEFAULT_BOS_TOKEN,\n","        \"unk_token\": DEFAULT_UNK_TOKEN,\n","    }\n",")\n","tokenizer.pad_token = tokenizer.eos_token\n","print(tokenizer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"Q5KWEpUieycL","executionInfo":{"status":"ok","timestamp":1715530860845,"user_tz":-540,"elapsed":13111,"user":{"displayName":"ê¹€ì˜ˆë‘","userId":"06765726933479701190"}},"outputId":"307f94b1-8e8e-4fba-91b9-ca1d420a3606"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["GPT2TokenizerFast(name_or_path='skt/kogpt2-base-v2', vocab_size=51200, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t1: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t2: AddedToken(\"<usr>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t3: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t4: AddedToken(\"<sys>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t5: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t6: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t7: AddedToken(\"<d>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t8: AddedToken(\"</d>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t9: AddedToken(\"<unused0>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t10: AddedToken(\"<unused1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t11: AddedToken(\"<unused2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t12: AddedToken(\"<unused3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t13: AddedToken(\"<unused4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t14: AddedToken(\"<unused5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t15: AddedToken(\"<unused6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t16: AddedToken(\"<unused7>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t17: AddedToken(\"<unused8>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t18: AddedToken(\"<unused9>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t19: AddedToken(\"<unused10>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t20: AddedToken(\"<unused11>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t21: AddedToken(\"<unused12>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t22: AddedToken(\"<unused13>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t23: AddedToken(\"<unused14>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t24: AddedToken(\"<unused15>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t25: AddedToken(\"<unused16>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t26: AddedToken(\"<unused17>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t27: AddedToken(\"<unused18>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t28: AddedToken(\"<unused19>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t29: AddedToken(\"<unused20>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t30: AddedToken(\"<unused21>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t31: AddedToken(\"<unused22>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t32: AddedToken(\"<unused23>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t33: AddedToken(\"<unused24>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t34: AddedToken(\"<unused25>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t35: AddedToken(\"<unused26>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t36: AddedToken(\"<unused27>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t37: AddedToken(\"<unused28>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t38: AddedToken(\"<unused29>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t39: AddedToken(\"<unused30>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t40: AddedToken(\"<unused31>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t41: AddedToken(\"<unused32>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t42: AddedToken(\"<unused33>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t43: AddedToken(\"<unused34>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t44: AddedToken(\"<unused35>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t45: AddedToken(\"<unused36>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t46: AddedToken(\"<unused37>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t47: AddedToken(\"<unused38>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t48: AddedToken(\"<unused39>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t49: AddedToken(\"<unused40>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50: AddedToken(\"<unused41>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t51: AddedToken(\"<unused42>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t52: AddedToken(\"<unused43>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t53: AddedToken(\"<unused44>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t54: AddedToken(\"<unused45>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t55: AddedToken(\"<unused46>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t56: AddedToken(\"<unused47>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t57: AddedToken(\"<unused48>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t58: AddedToken(\"<unused49>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t59: AddedToken(\"<unused50>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t60: AddedToken(\"<unused51>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t61: AddedToken(\"<unused52>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t62: AddedToken(\"<unused53>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t63: AddedToken(\"<unused54>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t64: AddedToken(\"<unused55>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t65: AddedToken(\"<unused56>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t66: AddedToken(\"<unused57>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t67: AddedToken(\"<unused58>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t68: AddedToken(\"<unused59>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t69: AddedToken(\"<unused60>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t70: AddedToken(\"<unused61>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t71: AddedToken(\"<unused62>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t72: AddedToken(\"<unused63>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t73: AddedToken(\"<unused64>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t74: AddedToken(\"<unused65>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t75: AddedToken(\"<unused66>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t76: AddedToken(\"<unused67>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t77: AddedToken(\"<unused68>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t78: AddedToken(\"<unused69>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t79: AddedToken(\"<unused70>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t80: AddedToken(\"<unused71>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t81: AddedToken(\"<unused72>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t82: AddedToken(\"<unused73>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t83: AddedToken(\"<unused74>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t84: AddedToken(\"<unused75>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t85: AddedToken(\"<unused76>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t86: AddedToken(\"<unused77>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t87: AddedToken(\"<unused78>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t88: AddedToken(\"<unused79>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t89: AddedToken(\"<unused80>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t90: AddedToken(\"<unused81>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t91: AddedToken(\"<unused82>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t92: AddedToken(\"<unused83>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t93: AddedToken(\"<unused84>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t94: AddedToken(\"<unused85>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t95: AddedToken(\"<unused86>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t96: AddedToken(\"<unused87>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t97: AddedToken(\"<unused88>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t98: AddedToken(\"<unused89>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t99: AddedToken(\"<unused90>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t100: AddedToken(\"<unused91>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t101: AddedToken(\"<unused92>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t102: AddedToken(\"<unused93>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t103: AddedToken(\"<unused94>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t104: AddedToken(\"<unused95>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t105: AddedToken(\"<unused96>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t106: AddedToken(\"<unused97>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t107: AddedToken(\"<unused98>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t108: AddedToken(\"<unused99>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t109: AddedToken(\":-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t110: AddedToken(\":)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t111: AddedToken(\"-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t112: AddedToken(\"(-:\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t113: AddedToken(\"(:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t114: AddedToken(\"(:-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t115: AddedToken(\"-}\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t116: AddedToken(\"8-O\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t117: AddedToken(\"'-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t118: AddedToken(\":-#\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t119: AddedToken(\":-*\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t120: AddedToken(\":-/\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t121: AddedToken(\":->\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t122: AddedToken(\":-@\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t123: AddedToken(\":-d\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t124: AddedToken(\":-V\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t125: AddedToken(\":-X\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t126: AddedToken(\":-\\\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t127: AddedToken(\":-]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t128: AddedToken(\";-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t129: AddedToken(\">;->\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t130: AddedToken(\";^)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t131: AddedToken(\"%-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t132: AddedToken(\"):-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t133: AddedToken(\"3:]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t134: AddedToken(\":-&\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t135: AddedToken(\"8:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t136: AddedToken(\":-)8<\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t137: AddedToken(\":-O\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t138: AddedToken(\":-6\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t139: AddedToken(\"+:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t140: AddedToken(\"O:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t141: AddedToken(\":-<\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t142: AddedToken(\":-?\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t143: AddedToken(\":-E\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t144: AddedToken(\":-Q\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t145: AddedToken(\":-}X\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t146: AddedToken(\":-[\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t147: AddedToken(\":-a\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t148: AddedToken(\":-{\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t149: AddedToken(\":-{}\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t150: AddedToken(\":^)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t151: AddedToken(\"<:-l\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t152: AddedToken(\":=)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t153: AddedToken(\">:->\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t154: AddedToken(\">:-l\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t155: AddedToken(\"@:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t156: AddedToken(\"@:-}\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t157: AddedToken(\"C=:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t158: AddedToken(\"X:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t159: AddedToken(\"[:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t160: AddedToken(\"[:]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t161: AddedToken(\"{:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t162: AddedToken(\"l^o\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t163: AddedToken(\"}:^#)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t164: AddedToken(\":-(=)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t165: AddedToken(\"O-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t166: AddedToken(\":-3\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t167: AddedToken(\":=\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t168: AddedToken(\":-\"\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t169: AddedToken(\"P-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t170: AddedToken(\"?-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t171: AddedToken(\"d:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t172: AddedToken(\":8)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t173: AddedToken(\":-7\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t174: AddedToken(\"):-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t175: AddedToken(\":/\\)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t176: AddedToken(\"8(:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t177: AddedToken(\"([(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t178: AddedToken(\":-(*)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t179: AddedToken(\"&-l\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t180: AddedToken(\":-e\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t181: AddedToken(\":(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t182: AddedToken(\":,(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t183: AddedToken(\":-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t184: AddedToken(\":-P\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t185: AddedToken(\":-S\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t186: AddedToken(\":-C\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t187: AddedToken(\":-r\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t188: AddedToken(\":-t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t189: AddedToken(\":-W\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t190: AddedToken(\"X-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t191: AddedToken(\"l-O\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t192: AddedToken(\"l:-O\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t193: AddedToken(\"$-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t194: AddedToken(\":-!\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t195: AddedToken(\":----}\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t196: AddedToken(\"=:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t197: AddedToken(\"=:-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t198: AddedToken(\"3:[\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t199: AddedToken(\"8<:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t200: AddedToken(\":#)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t201: AddedToken(\"8-#\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t202: AddedToken(\"B-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t203: AddedToken(\"8-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t204: AddedToken(\"|-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t205: AddedToken(\"H-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t206: AddedToken(\"]-I\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t207: AddedToken(\"V^J\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t208: AddedToken(\"+-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t209: AddedToken(\"~:-P\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t210: AddedToken(\"`'\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t211: AddedToken(\"L-P\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t212: AddedToken(\"BI\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t213: AddedToken(\"O|\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t214: AddedToken(\"^^\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t215: AddedToken(\"ã…œã…œ\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t216: AddedToken(\"ã… ã… \", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t217: AddedToken(\"ã…¡ã…¡\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t218: AddedToken(\"ğŸ˜ \", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t219: AddedToken(\"ğŸ‘¿\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t220: AddedToken(\"ğŸ˜§\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t221: AddedToken(\"ğŸ˜°\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t222: AddedToken(\"ğŸ˜²\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t223: AddedToken(\"ğŸ˜\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t224: AddedToken(\"ğŸ»\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t225: AddedToken(\"ğŸ±\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t226: AddedToken(\"ğŸ˜¹\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t227: AddedToken(\"ğŸ˜¼\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t228: AddedToken(\"ğŸ¤¡\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t229: AddedToken(\"ğŸ¥¶\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t230: AddedToken(\"ğŸ˜–\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t231: AddedToken(\"ğŸ˜•\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t232: AddedToken(\"ğŸ®\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t233: AddedToken(\"ğŸ¤ \", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t234: AddedToken(\"ğŸ˜¿\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t235: AddedToken(\"ğŸ˜¢\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t236: AddedToken(\"ğŸ˜\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t237: AddedToken(\"ğŸ˜µ\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t238: AddedToken(\"ğŸ¶\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t239: AddedToken(\"ğŸ˜“\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t240: AddedToken(\"ğŸ²\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t241: AddedToken(\"ğŸ¤¤\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t242: AddedToken(\"ğŸ˜‘\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t243: AddedToken(\"ğŸ˜˜\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t244: AddedToken(\"ğŸ˜‹\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t245: AddedToken(\"ğŸ˜±\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t246: AddedToken(\"ğŸ¤®\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t247: AddedToken(\"ğŸ¤­\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t248: AddedToken(\"ğŸ¤•\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t249: AddedToken(\"ğŸ˜·\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t250: AddedToken(\"ğŸ§\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t251: AddedToken(\"ğŸ˜®\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t252: AddedToken(\"ğŸ¤¨\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t253: AddedToken(\"ğŸ™„\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t254: AddedToken(\"ğŸ˜¤\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t255: AddedToken(\"ğŸ¤¬\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t256: AddedToken(\"ğŸ˜‚\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t257: AddedToken(\"ğŸ¤’\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t258: AddedToken(\"ğŸ˜›\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t259: AddedToken(\"ğŸ˜¶\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t260: AddedToken(\"ğŸ˜¨\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t261: AddedToken(\"ğŸŒ›\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t262: AddedToken(\"ğŸ˜³\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t263: AddedToken(\"ğŸ¦Š\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t264: AddedToken(\"ğŸ¸\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t265: AddedToken(\"â˜¹\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t266: AddedToken(\"â˜¹ï¸\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t267: AddedToken(\"ğŸ˜¦\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t268: AddedToken(\"ğŸŒ\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t269: AddedToken(\"ğŸ˜¬\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t270: AddedToken(\"ğŸ˜º\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t271: AddedToken(\"ğŸ˜¸\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t272: AddedToken(\"ğŸ˜€\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t273: AddedToken(\"ğŸ˜ƒ\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t274: AddedToken(\"ğŸ˜„\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t275: AddedToken(\"ğŸ˜…\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t276: AddedToken(\"ğŸ˜†\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t277: AddedToken(\"ğŸ¹\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t278: AddedToken(\"ğŸ´\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t279: AddedToken(\"ğŸ¥µ\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t280: AddedToken(\"ğŸ¤—\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t281: AddedToken(\"ğŸ˜¯\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t282: AddedToken(\"ğŸ˜½\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t283: AddedToken(\"ğŸ˜—\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t284: AddedToken(\"ğŸ˜š\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t285: AddedToken(\"ğŸ˜™\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t286: AddedToken(\"ğŸŒœ\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t287: AddedToken(\"ğŸ¦\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t288: AddedToken(\"ğŸ˜­\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t289: AddedToken(\"ğŸ¤¥\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t290: AddedToken(\"ğŸ¤¦ğŸ¿â€â™‚\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t291: AddedToken(\"ğŸ¤¦ğŸ»â€â™‚\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t292: AddedToken(\"ğŸ¤¦ğŸ¾â€â™‚\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t293: AddedToken(\"ğŸ¤¦ğŸ¼â€â™‚\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t294: AddedToken(\"ğŸ¤¦ğŸ½â€â™‚\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t295: AddedToken(\"ğŸ¤¦â€â™‚\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t296: AddedToken(\"ğŸ¤¦ğŸ¿â€â™‚ï¸\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t297: AddedToken(\"ğŸ¤¦ğŸ»â€â™‚ï¸\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t298: AddedToken(\"ğŸ¤¦ğŸ¾â€â™‚ï¸\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t299: AddedToken(\"ğŸ¤¦ğŸ¼â€â™‚ï¸\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t300: AddedToken(\"ğŸ¤¦ğŸ½â€â™‚ï¸\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t301: AddedToken(\"ğŸ¤¦â€â™‚ï¸\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t302: AddedToken(\"ğŸ¤‘\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t303: AddedToken(\"ğŸµ\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t304: AddedToken(\"ğŸ­\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t305: AddedToken(\"ğŸ¤¢\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t306: AddedToken(\"ğŸ¤“\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t307: AddedToken(\"ğŸ˜\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t308: AddedToken(\"ğŸŒš\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t309: AddedToken(\"ğŸ¼\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t310: AddedToken(\"ğŸ¥³\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t311: AddedToken(\"ğŸ˜”\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t312: AddedToken(\"ğŸ˜£\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t313: AddedToken(\"ğŸ¤¦\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t314: AddedToken(\"ğŸ¤¦ğŸ¿\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t315: AddedToken(\"ğŸ¤¦ğŸ»\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t316: AddedToken(\"ğŸ¤¦ğŸ¾\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t317: AddedToken(\"ğŸ¤¦ğŸ¼\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t318: AddedToken(\"ğŸ¤¦ğŸ½\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t319: AddedToken(\"ğŸ·\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t320: AddedToken(\"ğŸ¥º\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t321: AddedToken(\"ğŸ˜¾\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t322: AddedToken(\"ğŸ˜¡\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t323: AddedToken(\"ğŸ°\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t324: AddedToken(\"ğŸ˜Œ\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t325: AddedToken(\"ğŸ¤–\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t326: AddedToken(\"ğŸ˜¥\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t327: AddedToken(\"ğŸ¤«\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t328: AddedToken(\"ğŸ˜´\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t329: AddedToken(\"ğŸ˜ª\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t330: AddedToken(\"ğŸ™\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t331: AddedToken(\"ğŸ™‚\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t332: AddedToken(\"ğŸ˜»\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t333: AddedToken(\"â˜º\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t334: AddedToken(\"â˜ºï¸\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t335: AddedToken(\"ğŸ¥°\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t336: AddedToken(\"ğŸ˜‡\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t337: AddedToken(\"ğŸ˜\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t338: AddedToken(\"ğŸ˜ˆ\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t339: AddedToken(\"ğŸ˜Š\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t340: AddedToken(\"ğŸ˜\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t341: AddedToken(\"ğŸ˜\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t342: AddedToken(\"ğŸ¤§\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t343: AddedToken(\"ğŸ˜\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t344: AddedToken(\"ğŸŒ\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t345: AddedToken(\"ğŸ¤”\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t346: AddedToken(\"ğŸ¯\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t347: AddedToken(\"ğŸ˜«\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t348: AddedToken(\"ğŸ˜’\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t349: AddedToken(\"ğŸ¦„\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t350: AddedToken(\"ğŸ™ƒ\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t351: AddedToken(\"ğŸ™€\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t352: AddedToken(\"ğŸ˜©\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t353: AddedToken(\"ğŸŒ¬\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t354: AddedToken(\"ğŸŒ¬ï¸\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t355: AddedToken(\"ğŸ˜‰\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t356: AddedToken(\"ğŸ˜œ\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t357: AddedToken(\"ğŸº\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t358: AddedToken(\"ğŸ¤¦ğŸ¿â€â™€\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t359: AddedToken(\"ğŸ¤¦ğŸ»â€â™€\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t360: AddedToken(\"ğŸ¤¦ğŸ¾â€â™€\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t361: AddedToken(\"ğŸ¤¦ğŸ¼â€â™€\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t362: AddedToken(\"ğŸ¤¦ğŸ½â€â™€\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t363: AddedToken(\"ğŸ¤¦â€â™€\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t364: AddedToken(\"ğŸ¤¦ğŸ¿â€â™€ï¸\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t365: AddedToken(\"ğŸ¤¦ğŸ»â€â™€ï¸\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t366: AddedToken(\"ğŸ¤¦ğŸ¾â€â™€ï¸\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t367: AddedToken(\"ğŸ¤¦ğŸ¼â€â™€ï¸\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t368: AddedToken(\"ğŸ¤¦ğŸ½â€â™€ï¸\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t369: AddedToken(\"ğŸ¤¦â€â™€ï¸\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t370: AddedToken(\"ğŸ¥´\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t371: AddedToken(\"ğŸ˜Ÿ\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t372: AddedToken(\"ğŸ¥±\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t373: AddedToken(\"ğŸ¤ª\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t374: AddedToken(\"ğŸ¤\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t51200: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}\n"]}]},{"cell_type":"markdown","source":["###SFTë¥¼ ìœ„í•œ ë°ì´í„°ì…‹ ì¤€ë¹„"],"metadata":{"id":"-b1b2UiafjfY"}},{"cell_type":"code","source":["from typing import Optional, Dict, Sequence\n","\n","class SFT_dataset(Dataset):\n","    '''SFT dataset by wygo'''\n","    def __init__(self, data_path_1_SFT: str, tokenizer: transformers.PreTrainedTokenizer, verbose=False):\n","        super(SFT_dataset, self).__init__()\n","        logging.warning(\"Loading data...\")\n","\n","        ## format\n","        pattern_instruction = 'prompt'  # ìˆœí™”ë¥¼ ì§€ì‹œí•˜ëŠ” ëª…ë ¹ (í”„ë¡¬í”„íŠ¸)\n","        pattern_input = 'input'  # ìˆœí™”ì‹œí‚¬ ë°ì´í„°\n","        pattern_output = 'completion'  # ìˆœí™”ëœ output\n","\n","        ############################################################\n","        ## load dataset\n","        # ë‚´ ë°ì´í„°ì…‹ì—” inputì´ ì—†ë‹¤ >> KoMoíŒ€ì€ ì•„ë‹ˆì§€ë¡± ìˆì§€ë¡±\n","        data_path_1_SFT = '/content/drive/MyDrive/combined_final_file.csv'\n","        list_data_csv = pd.read_csv(data_path_1_SFT, encoding='utf-8')\n","        profanity_data = list_data_csv['Profanity']\n","        cleaned_data = list_data_csv['Cleaned']\n","        if True:\n","          print(profanity_data[0])\n","        # with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n","        #     list_data_dict = json.load(json_file)\n","        #     if True:\n","        #       print(list_data_dict)\n","\n","        ############################################################\n","        ## ë°ì´í„°ì…‹ ë§Œë“¤ê¸°, sourceì™€ target\n","        prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]  # í…œí”Œë¦¿ ê°€ì ¸ì˜¤ê¸°\n","\n","        # ì…ë ¥\n","        sources = []\n","        for example in profanity_data:\n","            if example != \"\":\n","                tmp = prompt_no_input.format(example) if isinstance(example, str) else prompt_no_input\n","            else:\n","                tmp = prompt_input\n","            sources.append(tmp)\n","\n","        # ì¶œë ¥\n","        targets = []\n","        for example in cleaned_data:\n","            targets.append(f\"{example}{tokenizer.eos_token}\" if isinstance(example, str) else f\"{tokenizer.eos_token}\")\n","\n","        if verbose:\n","            idx = 0\n","            print(sources[idx])\n","            print(targets[idx])\n","            print(\"Tokenizing inputs... This may take some time...\")\n","\n","        ############################################################\n","        # data_dict = preprocess(sources, targets, tokenizer)  # https://github.com/Beomi/KoAlpaca/blob/04704348d58b8b1c2e2638d6437a04b4e8ba1823/train.py#L124\n","        examples = [s + t for s, t in zip(sources, targets)]\n","\n","        # source data tokenized\n","        sources_tokenized = self._tokenize_fn(sources, tokenizer)  # sourceë§Œ\n","        examples_tokenized = self._tokenize_fn(examples, tokenizer)  # source + target\n","\n","\n","        ## ì…ë ¥ì€ source, ì¶œë ¥ì€ source+target ì´ì§€ë§Œ í•™ìŠµì€ target ë¶€ë¶„ë§Œ\n","        input_ids = examples_tokenized[\"input_ids\"]\n","        labels = copy.deepcopy(input_ids)\n","        for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n","            label[:source_len] = IGNORE_INDEX  # source ë¶€ë¶„ì€ -100ìœ¼ë¡œ ì±„ìš´ë‹¤\n","\n","        data_dict = dict(input_ids=input_ids, labels=labels)\n","\n","        self.input_ids = data_dict[\"input_ids\"]\n","        self.labels = data_dict[\"labels\"]\n","        logging.warning(\"Loading data done!!: %d\"%(len(self.labels)))\n","\n","    def _tokenize_fn(self, strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n","        \"\"\"Tokenize a list of strings.\"\"\"\n","        tokenized_list = [\n","            tokenizer(\n","                text,\n","                return_tensors=\"pt\",\n","                padding=\"longest\",\n","                max_length=tokenizer.model_max_length,\n","                truncation=True,\n","            )\n","            for text in strings\n","        ]\n","        input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n","        input_ids_lens = labels_lens = [\n","            tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n","        ]\n","        return dict(\n","            input_ids=input_ids,\n","            labels=labels,\n","            input_ids_lens=input_ids_lens,\n","            labels_lens=labels_lens,\n","        )\n","\n","\n","    def __len__(self):\n","        return len(self.input_ids)\n","\n","\n","    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n","        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n","\n","\n","@dataclass\n","class DataCollatorForSupervisedDataset(object):\n","    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n","\n","    tokenizer: transformers.PreTrainedTokenizer\n","\n","    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n","        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n","        input_ids = torch.nn.utils.rnn.pad_sequence(\n","            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n","        )\n","        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n","        return dict(\n","            input_ids=input_ids,\n","            labels=labels,\n","            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n","        )\n","\n","\n","\n","train_dataset = SFT_dataset(data_path_1_SFT=args.data_path_1_SFT, tokenizer=tokenizer)\n","eval_dataset  = None  # evalì€ ì•ˆí•¨\n","data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n","\n","# check\n","print('input : %s'%train_dataset.input_ids[0])\n","print('output: %s'%train_dataset.labels[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3GoVsw9ZezsV","executionInfo":{"status":"ok","timestamp":1715530883168,"user_tz":-540,"elapsed":22326,"user":{"displayName":"ê¹€ì˜ˆë‘","userId":"06765726933479701190"}},"outputId":"b6865eab-7549-4790-a9a5-a1399faa655a"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:root:Loading data...\n"]},{"output_type":"stream","name":"stdout","text":["ê·¸ëƒ¥ ê²Œì„ì´ë‚˜ í•˜ìê³ ? ã…‹ã…‹ ë‹ˆê°€ ê²Œì„í•˜ëŠ”ê±° ë³´ë©´ ì•”ê±¸ë¦´ê±°ê°™ì•„ì„œ ì°¨ë¼ë¦¬ ë‹ˆ í˜¼ì ê²Œì„í•´ë¼ ã……ã…‚ ë‹ˆë‘ ê°™ì´ í•˜ê¸° ì‹«ìœ¼ë‹ˆê¹Œ êº¼ì ¸\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:Loading data done!!: 23533\n"]},{"output_type":"stream","name":"stdout","text":["input : tensor([12160, 16122, 20600,  9793,  9241,  9492, 12160, 16122,  9712, 41775,\n","        34407, 43257, 27729,  8244,   389,  9793,  9182, 11909, 11522,  9038,\n","        27318, 31618, 10669,  9314, 11451, 25243,  9659,  8244,   375,   375,\n","          452,  9610,   454, 14197,  9223, 40972, 12371,  9208,  6889,  8615,\n","          468,   468,   375,   375, 44682,   454, 14197,  9223,  9208,  6889,\n","         8688,  7119, 13779,  7991, 14380,   468,   375,   375,  9610,   454,\n","        14197,  9223, 28257, 12371, 27080,  9078,  9495, 16238,   375,   375,\n","        44682,   454, 14197,  9223,  9755,  7235, 27457, 13643,  6866,  9760,\n","        37932, 29777,  7119,  7556,  9564,  7071,  7788,  9078,  9495, 15254,\n","         7119,   468,   739,  9585,   375,   378,   378,   378, 14659, 13394,\n","        37091, 10651,   383, 25841,  8006, 14914,   375,   452, 36420,   375,\n","          375,   378,   378,   378, 41951,   454,  9549, 20549,   383,  8142,\n","         7192, 14914,  6947,  7084, 15403,  9185, 17709,   389, 14927,  9779,\n","        17537,  6958, 15254, 13083,     1])\n","output: tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  6947,  7084, 15403,  9185, 17709,   389, 14927,  9779,\n","        17537,  6958, 15254, 13083,     1])\n"]}]},{"cell_type":"code","source":["## í•™ìŠµ (10min)\n","# training_args ìˆ˜ì • ê°€ëŠ¥: https://github.com/Beomi/KoAlpaca/blob/main/train.sh ì°¸ê³ \n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(device)\n","training_args = TrainingArguments(\n","    output_dir=\"./sft_test\", #The output directory\n","    overwrite_output_dir=True, #overwrite the content of the output directory\n","    num_train_epochs=1, # number of training epochs\n","    per_device_train_batch_size=4, # batch size for training\n","    per_device_eval_batch_size=4,  # batch size for evaluation\n","    eval_steps = 3, # Number of update steps between two evaluations.\n","    save_steps=500, # after # steps model is saved\n","    warmup_steps=5,# number of warmup steps for learning rate scheduler\n","    prediction_loss_only=True,\n","    )\n","trainer = Trainer(\n","    model=model.to(device),\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n",")\n","\n","trainer.train()\n","trainer.save_state()\n","safe_save_model_for_hf_trainer(trainer=trainer, output_dir=args.output_dir)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":450},"id":"fF3GJA6dgJsP","executionInfo":{"status":"error","timestamp":1715520550307,"user_tz":-540,"elapsed":16573,"user":{"displayName":"ê¹€ì˜ˆë‘","userId":"06765726933479701190"}},"outputId":"af96213f-18a2-4da5-c5ed-a79ce7dd7817","collapsed":true},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2' max='5884' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [   2/5884 : < :, Epoch 0.00/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-46-b81f828826db>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0msafe_save_model_for_hf_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1555\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1556\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1557\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1859\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1860\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m                 if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2724\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2725\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2727\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2746\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2747\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2748\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2749\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2750\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1094\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m         \u001b[0mlm_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["##Reward Model"],"metadata":{"id":"7r9rVR2T2uOG"}},{"cell_type":"code","source":["# import\n","import argparse\n","\n","import loralib as lora\n","import torch\n","torch.cuda.empty_cache()\n","from chatgpt.dataset import RewardDataset\n","from chatgpt.models.base import RewardModel\n","from chatgpt.models.bloom import BLOOMRM\n","from chatgpt.models.gpt import GPTRM\n","from chatgpt.models.opt import OPTRM\n","from chatgpt.trainer import RewardModelTrainer\n","from chatgpt.trainer.strategies import ColossalAIStrategy, DDPStrategy, NaiveStrategy\n","from datasets import load_dataset\n","from torch.optim import Adam\n","from transformers import AutoTokenizer, BloomTokenizerFast\n","from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n","\n","from colossalai.nn.optimizer import HybridAdam\n","\n","import os\n","import json\n","\n","# data config\n","IGNORE_INDEX = -100\n","DEFAULT_PAD_TOKEN = \"[PAD]\"\n","DEFAULT_EOS_TOKEN = \"</s>\"\n","DEFAULT_BOS_TOKEN = \"</s>\"\n","\n","# define argment\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--output_dir', type=str, default='./output_2_RM')\n","parser.add_argument('--data_path_2_RM', type=str, default='./data_kochatgpt/kochatgpt_2_RM.jsonl', help='https://huggingface.co/datasets/fka/awesome-chatgpt-prompts/blob/main/prompts.csv')\n","parser.add_argument('--strategy',\n","                    choices=['naive', 'ddp', 'colossalai_gemini', 'colossalai_zero2'],\n","                    default='naive')\n","parser.add_argument('--model', type=str, default='gpt2', choices=['gpt2', 'bloom', 'opt'])\n","parser.add_argument('--pretrain', type=str, default=None)\n","parser.add_argument('--dataset', type=str, default='Dahoas/rm-static')\n","parser.add_argument('--save_path', type=str, default='rm_ckpt.pth')\n","parser.add_argument('--max_epochs', type=int, default=10)\n","parser.add_argument('--batch_size', type=int, default=4)\n","parser.add_argument('--lora_rank', type=int, default=0, help=\"low-rank adaptation matrices rank\")\n","parser.add_argument('--max_len', type=int, default=512)  # wygo ì¶”ê°€\n","\n","args = parser.parse_args(args=[])\n","\n","# for test\n","args.max_epochs = 3\n","args.pretrain = 'skt/kogpt2-base-v2'  # pretrained ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°\n","args.verbose = True\n","\n","print(args)\n","if not os.path.exists(args.output_dir):\n","    os.makedirs(args.output_dir)\n","\n","# configure strategy\n","if args.strategy == 'naive':\n","    strategy = NaiveStrategy()\n","elif args.strategy == 'ddp':\n","    strategy = DDPStrategy()\n","elif args.strategy == 'colossalai_gemini':\n","    strategy = ColossalAIStrategy(stage=3, placement_policy='cuda')\n","elif args.strategy == 'colossalai_zero2':\n","    strategy = ColossalAIStrategy(stage=2, placement_policy='cuda')\n","else:\n","    raise ValueError(f'Unsupported strategy \"{args.strategy}\"')"],"metadata":{"id":"VxNrhTjz0MhO","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1715530908450,"user_tz":-540,"elapsed":1415,"user":{"displayName":"ê¹€ì˜ˆë‘","userId":"06765726933479701190"}},"outputId":"aa4bd21e-1582-4817-b42b-6516db55b2aa"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/library.py:130: UserWarning: Overriding a previously registered kernel for the same operator and the same dispatch key\n","  operator: aten::index.Tensor(Tensor self, Tensor?[] indices) -> Tensor\n","    registered at aten/src/ATen/RegisterSchema.cpp:6\n","  dispatch key: Meta\n","  previous kernel: registered at ../aten/src/ATen/functorch/BatchRulesScatterOps.cpp:1053\n","       new kernel: registered at /dev/null:241 (Triggered internally at ../aten/src/ATen/core/dispatch/OperatorEntry.cpp:150.)\n","  self.m.impl(name, dispatch_key, fn)\n"]},{"output_type":"stream","name":"stdout","text":["Namespace(output_dir='./output_2_RM', data_path_2_RM='./data_kochatgpt/kochatgpt_2_RM.jsonl', strategy='naive', model='gpt2', pretrain='skt/kogpt2-base-v2', dataset='Dahoas/rm-static', save_path='rm_ckpt.pth', max_epochs=3, batch_size=4, lora_rank=0, max_len=512, verbose=True)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:202: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n","  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"]}]},{"cell_type":"code","source":["# customizing, https://github.com/hpcaitech/ColossalAI/blob/2e16f842a9e5b1fb54e7e41070e9d2bb5cd64d7c/applications/ChatGPT/chatgpt/nn/gpt_rm.py#L29\n","from typing import Optional\n","\n","import torch.nn as nn\n","from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n","from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n","\n","# from ..base import RewardModel\n","from chatgpt.models.base import RewardModel\n","\n","\n","class GPTRM_custom(RewardModel):\n","    \"\"\"\n","    GPT Reward model.\n","    Args:\n","        pretrained (str): Pretrained model name or path.\n","        config (GPT2Config): Model config.\n","        checkpoint (bool): Enable gradient checkpointing.\n","        lora_rank (int): Rank of the low-rank approximation.\n","        lora_train_bias (str): LoRA bias training mode.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 pretrained: Optional[str] = None,\n","                 config: Optional[GPT2Config] = None,\n","                 checkpoint: bool = False,\n","                 lora_rank: int = 0,\n","                 lora_train_bias: str = 'none',\n","                 tokenizer=None) -> None:\n","        if pretrained is not None:\n","            model = GPT2Model.from_pretrained(pretrained)\n","            model.resize_token_embeddings(len(tokenizer))  # wygo ì¶”ê°€!!!\n","        elif config is not None:\n","            model = GPT2Model(config)\n","        else:\n","            model = GPT2Model(GPT2Config())\n","        if checkpoint:\n","            model.gradient_checkpointing_enable()\n","\n","\n","        # model = model.resize_token_embeddings(len(tokenizer))\n","\n","        value_head = nn.Linear(model.config.n_embd, 1)\n","        super().__init__(model, value_head, lora_rank, lora_train_bias)\n","\n","        # ì¶”ê°€, 230421\n","        if pretrained is not None:\n","            self.model = model\n","            self.pretrained = pretrained\n","\n","    # ì¶”ê°€, 230421, config.jsonì„ ìƒì„±í•˜ê¸° ìœ„í•´ ì¶”ê°€\n","    def save_pretrained(self, dir):\n","        if self.pretrained is not None:\n","            self.model.save_pretrained(dir)"],"metadata":{"id":"k0I7Udr4wgd1","executionInfo":{"status":"ok","timestamp":1715533412723,"user_tz":-540,"elapsed":494,"user":{"displayName":"ê¹€ì˜ˆë‘","userId":"06765726933479701190"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["# configure model, tokenizer\n","with strategy.model_init_context():\n","    # load pretrained gpt2\n","    if args.model == 'gpt2':\n","#         tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","        # tokenizer = AutoTokenizer.from_pretrained(args.pretrain)\n","        tokenizer = AutoTokenizer.from_pretrained(args.pretrain, padding_side=\"right\", model_max_length=512)\n","        tokenizer.add_special_tokens(\n","            {\n","                \"eos_token\": DEFAULT_EOS_TOKEN,\n","                \"bos_token\": DEFAULT_BOS_TOKEN,\n","                \"unk_token\": DEFAULT_UNK_TOKEN,\n","            }\n","        )\n","        tokenizer.pad_token = tokenizer.eos_token\n","        model = GPTRM_custom(pretrained=args.pretrain, lora_rank=args.lora_rank, tokenizer=tokenizer) #.cuda() ë¶™ì—¬ì„œ ì‹¤í–‰í•˜ê¸°\n","\n","    elif args.model == 'bloom':\n","        model = BLOOMRM(pretrained=args.pretrain, lora_rank=args.lora_rank).cuda()\n","        tokenizer = BloomTokenizerFast.from_pretrained(args.pretrain)\n","\n","    elif args.model == 'opt':\n","        model = OPTRM(pretrained=args.pretrain, lora_rank=args.lora_rank).cuda()\n","        tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n","\n","    else:\n","        raise ValueError(f'Unsupported model \"{args.model}\"')\n","\n","\n","    # model.resize_token_embeddings(len(tokenizer))"],"metadata":{"id":"9qe8mVJ9wmpo","executionInfo":{"status":"ok","timestamp":1715533448048,"user_tz":-540,"elapsed":9973,"user":{"displayName":"ê¹€ì˜ˆë‘","userId":"06765726933479701190"}}},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":["##Reward Model ë°ì´í„° í˜•ì‹\n","\n","{'prompt': 'ë©”ë‰´íŒ ìˆì–´ìš”?',\n","'completion_0': 'ì£„ì†¡í•©ë‹ˆë‹¤, ì œê°€ ê°€ìƒì˜ ì±—ë´‡ì…ë‹ˆë‹¤. ì œê°€ ë©”ë‰´íŒì„ ì œê³µí•  ìˆ˜ëŠ” ì—†ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ, í˜¹ì‹œ ê¶ê¸ˆí•œ ìŒì‹ì´ë‚˜ ìŒë£Œê°€ ìˆë‹¤ë©´ ì•Œë ¤ì£¼ì‹œë©´ ê·¸ì— ë§ëŠ” ì •ë³´ë¥¼ ì œê³µí•´ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.',\n","\n","'completion_1': 'ì´ë½ìœ¼ë¡œ ì˜ì•Œë©´ ê°•ì˜í•  ì§€ ë“± ê³µê°œí•œ ê²½ê°œëŠ” ë†’ì•„ ê°€ê³  í•œë‹¤ë©´ ê°ì²´ê°œë¥¼ ìœ„í•œ ê°œì„œë¥¼ í•œë‹¤ê°€ êµ­í† ë²•ì¸ ê²½ê°œë¥¼ ë†’ê²Œ ì†í•œ ê²½ê°œëŠ” ì¼ í•˜ë‚˜ êµ­í† ë²•ì¸ ê²½ê°œë¥¼ ë“¤ì—¬ ê°ì²´ê°œëŠ” ë” êµ­í† ë²•ì¸ ê²½ê°œë¥¼ ë†’ì´ í•œë‹¤ë©´ ê°ì²´ê°œë¥¼ í† ë²•ì¸ ê²½ê°œë¥¼ ë†’ì´ í•œë‹¤ê³  êµ­í† ë²•ì¸ ê²½ê°œë¥¼ í•œë‹¤ë©´ êµ­í† ë²•ì¸ ê²½ê°œë¥¼ ë†’ì´ í† ë²•ì¸ ê²½ê°œëŠ” êµ­í† ë²•ì¸ ê²½ê°œë¥¼ ë†’ì´ ê°ì²´ê°œë¥¼ í† ë²•ì¸ ê²½ê°œë¥¼ í•œë‹¤ë©´ êµ­í† ë²•ì¸ ê²½ê°œëŠ”ë‹¤\\n\\nêµ­í† ë²•ì¸ ê²½ê°œë¥¼ ë“¤ì—¬ êµ­í† ë²•ì¸ ê²½ê°œë¥¼ êµ­í† ë²•ì¸ ê²½ê°œë¥¼ ë†’ì´ êµ­í† ë²•ì¸ ê²½ê°œë¥¼ í•œë‹¤ë©´ êµ­í† ë²•ì¸ ê²½ê°œë¥¼ í•œë‹¤ë©´ êµ­í† ë²•ì¸ í† ë²•ì¸ ê²½ê°œë¥¼ ë†’ì´ í•œë‹¤ë©´ êµ­í† ë²•ì¸ ê²½ê°œë¥¼ ë†’ì´ í•œë‹¤ë©´ êµ­í† ë²•ì¸ ê²½ê°œë¥¼ í•œë‹¤ë©´ êµ­í† ë²•ì¸ ê²½ê°œë¥¼ ë†’ì´ í•œë‹¤ë©´ êµ­í† ë²•ì¸ ê²½ê°œë¥¼ í•œë‹¤ë©´ êµ­í† ë²•ì¸ ê²½ê°œë¥¼ í•œë‹¤',\n","\n","'completion_2': 'ë„¤, ì—¬ëŸ¬ê°€ì§€ ë©”ë‰´ê°€ ìˆìŠµë‹ˆë‹¤. ì¹˜í‚¨, í”¼ì, ìƒëŸ¬ë“œ, ìŠ¤íŒŒê²Œí‹°, ê·¸ë¦¬ê³  ë‹¤ì–‘í•œ ê°„ì‹ê³¼ ìŒë£Œ ë“±ì´ ìˆìŠµë‹ˆë‹¤.',\n","\n","'ranking': [0, 2, 1]}"],"metadata":{"id":"r-MwyeUSOnGY"}},{"cell_type":"markdown","source":["##Transfer KoMo data to RM data format"],"metadata":{"id":"KiS3B_LXPpoP"}},{"cell_type":"code","source":["import random\n","\n","rank0_path = '/content/drive/MyDrive/combined_emojis_final.csv' #ì›ë˜ë¬¸ì¥ì— ì´ëª¨ì§€ ìˆëŠ”,ê°€ì¥ ë­í¬ ë†’ì€ ë°ì´í„°\n","rank1_path = '/content/drive/MyDrive/original_conversation.json' #ì´ëª¨ì§€ ì—†ê³  ë¬¸ì¥ë§Œ ì°©í•œ ë¬¸ì¥ì¸ ì¤‘ê°„ ë­í¬ ë°ì´í„°\n","rank2_path = '/content/drive/MyDrive/combined_final_file.csv'#ìš•ì´ ë‚¨ë°œí•˜ëŠ” ê¼´ë“± ë°ì´í„°\n","\n","def read_json(path):\n","  with open(path,'r',encoding='utf-8') as f:\n","    data = json.load(f)\n","  return data\n","\n","def readcsv(path):\n","  data = pd.read_csv(path)\n","  return data\n","\n","def add_ranking_column(data, rank):\n","    data_with_rank = []\n","    for index, row in enumerate(data):\n","        data_dict = {\n","            'prompt': row,\n","            'ranking': rank\n","        }\n","        data_with_rank.append(data_dict)\n","    return data_with_rank\n","\n","rank0_data = readcsv(rank0_path)['Cleaned']\n","rank1_data = readcsv(rank2_path)['Cleaned'] #ì¼ë°˜ë¬¸ì¥ì€ ìš•ì„¤ ìˆœí™” ì „ ë¬¸ì¥ê³¼ ê°™ê¸° ë•Œë¬¸ì— rank2ì˜ 'cleaned'ë¡œ ì²˜ë¦¬\n","rank2_data = readcsv(rank2_path)['Profanity'] #ìš•ì„¤ ë°ì´í„°ì˜ 'profanity' ì—´ ê°’\n","\n","rank0_data_with_rank = add_ranking_column(rank0_data, 0)\n","rank1_data_with_rank = add_ranking_column(rank1_data, 1)\n","rank2_data_with_rank = add_ranking_column(rank2_data, 2)\n","\n","list_data_dict = []\n","for i,(r0,r1,r2) in enumerate(zip(rank0_data_with_rank,rank1_data_with_rank,rank2_data_with_rank)):\n","  combined_lists = [r0,r1,r2]\n","  random.shuffle(combined_lists)\n","  temp_dict = {\n","      'prompt':r1['prompt'],\n","      'completion_0':combined_lists[0]['prompt'],\n","      'completion_1':combined_lists[1]['prompt'],\n","      'completion_2':combined_lists[2]['prompt'],\n","      'ranking':[combined_lists[0]['ranking'],combined_lists[1]['ranking'],combined_lists[2]['ranking']]\n","  }\n","  list_data_dict.append(temp_dict)"],"metadata":{"id":"sZv3hO4kPt7X","executionInfo":{"status":"ok","timestamp":1715532556883,"user_tz":-540,"elapsed":1704,"user":{"displayName":"ê¹€ì˜ˆë‘","userId":"06765726933479701190"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["total_data_ranking2chosen = []\n","for tmp in list_data_dict:\n","    one_data_ranking2chosen = []\n","\n","    # data 1) 0 VS 1\n","    data = {}\n","    data['prompt'] = tmp['prompt']\n","    if tmp['ranking'][0] < tmp['ranking'][1]:\n","        data['chosen'] = str(tmp['completion_0'])\n","        data['rejected'] = tmp['completion_1']\n","    else:\n","        data['chosen'] = str(tmp['completion_1'])\n","        data['rejected'] = tmp['completion_0']\n","    one_data_ranking2chosen.append(data)\n","\n","\n","    # data 2) 0 VS 2\n","    data = {}\n","    data['prompt'] = tmp['prompt']\n","    if tmp['ranking'][0] < tmp['ranking'][2]:\n","        data['chosen'] = str(tmp['completion_0'])\n","        data['rejected'] = tmp['completion_2']\n","    else:\n","        data['chosen'] = str(tmp['completion_2'])\n","        data['rejected'] = tmp['completion_0']\n","    one_data_ranking2chosen.append(data)\n","\n","    # data 1) 1 VS 2\n","    data = {}\n","    data['prompt'] = tmp['prompt']\n","    if tmp['ranking'][1] < tmp['ranking'][2]:\n","        data['chosen'] = str(tmp['completion_1'])\n","        data['rejected'] = tmp['completion_2']\n","    else:\n","        data['chosen'] = str(tmp['completion_2'])\n","        data['rejected'] = tmp['completion_1']\n","    one_data_ranking2chosen.append(data)\n","\n","\n","\n","    total_data_ranking2chosen.extend(one_data_ranking2chosen)\n","\n","print('before data num: %d'%(len(list_data_dict)))\n","print('after  data num: %d'%(len(total_data_ranking2chosen)))\n","print('data example: \\n%s'%total_data_ranking2chosen[45])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0WfcoQe4_5p9","executionInfo":{"status":"ok","timestamp":1715533263188,"user_tz":-540,"elapsed":437,"user":{"displayName":"ê¹€ì˜ˆë‘","userId":"06765726933479701190"}},"outputId":"59fa1e35-8639-4185-cc9d-fa0bee5482a1"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["before data num: 20835\n","after  data num: 62505\n","data example: \n","{'prompt': 'ê²¨ìš° 2ê°œ? í•œíŒë„ ëª»í•¨ ì§„ì§œ', 'chosen': 'ê²¨ìš° 2ê°œ? í•œíŒë„ ëª»í•¨ ì§„ì§œ', 'rejected': 'ê²¨ìš° 2ê°œ ã…‹ã…‹ã…‹ í•œíŒì€ ë¬´ìŠ¨ ì”¹ë•ë“¤ì´ 2ê°œë¡œ ì˜¤ë°”í•˜ë„¤ ã…‰ã…‰'}\n"]}]},{"cell_type":"code","source":["print(total_data_ranking2chosen[45])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dYqk0XD4t8HR","executionInfo":{"status":"ok","timestamp":1715533267278,"user_tz":-540,"elapsed":433,"user":{"displayName":"ê¹€ì˜ˆë‘","userId":"06765726933479701190"}},"outputId":"91fd6ec1-caed-4b53-a978-aae675e9ea05"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["{'prompt': 'ê²¨ìš° 2ê°œ? í•œíŒë„ ëª»í•¨ ì§„ì§œ', 'chosen': 'ê²¨ìš° 2ê°œ? í•œíŒë„ ëª»í•¨ ì§„ì§œ', 'rejected': 'ê²¨ìš° 2ê°œ ã…‹ã…‹ã…‹ í•œíŒì€ ë¬´ìŠ¨ ì”¹ë•ë“¤ì´ 2ê°œë¡œ ì˜¤ë°”í•˜ë„¤ ã…‰ã…‰'}\n"]}]},{"cell_type":"code","source":["# prepare for data and dataset\n","import random\n","random.seed(230319)\n","# list_tmp = list(range(10))\n","random.shuffle(total_data_ranking2chosen)\n","print(total_data_ranking2chosen[45])\n","\n","# train_data = total_data_ranking2chosen[:-1000]  # 29000 í•™ìŠµ\n","# eval_data = total_data_ranking2chosen[-1000:0]  # 1000ê°œë§Œ í‰ê°€\n","\n","train_data = total_data_ranking2chosen[:100]  # 29000 í•™ìŠµ\n","eval_data = total_data_ranking2chosen[100:130]  # 1000ê°œë§Œ í‰ê°€\n","train_dataset = None\n","eval_dataset = None\n","for each in train_data:\n","  try:\n","    train_dataset = RewardDataset(each, tokenizer, args.max_len)\n","  except TypeError:\n","    continue\n","for each in eval_data:\n","  try:\n","    eval_dataset = RewardDataset(each, tokenizer, args.max_len)\n","  except TypeError:\n","      continue\n","\n","# check\n","# idx = 3\n","# print('#'*70)\n","# print('## prompt ##')\n","# print(train_data[idx]['prompt'])\n","# print('#'*70)\n","# print('## chosen ##')\n","# print(train_data[idx]['chosen'])\n","# print('#'*70)\n","# print('## rejected ##')\n","# print(train_data[idx]['rejected'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"qc7BzMF_t0p7","executionInfo":{"status":"ok","timestamp":1715533740982,"user_tz":-540,"elapsed":2092,"user":{"displayName":"ê¹€ì˜ˆë‘","userId":"06765726933479701190"}},"outputId":"0e1dd127-21a5-41f0-f344-06a3d65302a8"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["{'prompt': 'ë‚˜ ì˜ˆì „ì— ìœ„ì¥ ì•ˆ ì¢‹ì„ ë•ŒëŠ” ê·¸ëƒ¥ ì˜¤íŠ¸ë°€+ë”°ëœ»í•œ ë¬¼+ì„¤íƒ• í•´ì„œ ë¨¹ì—ˆëŠ”ë° ì´ê²Œ ì •ì„ ì•„ë‹Œê°€', 'chosen': 'ì´ì œ ë‚šì‹œ ê°ˆ ë•Œ ì©ë‹¤ë°°ë¦¬ ì°¨ íƒ€ê³  ê°€ì•¼ê² ì–´ ğŸ£ğŸš—', 'rejected': 'ë‚˜ ì˜ˆì „ì— ìœ„ì¥ ì•ˆ ì¢‹ì„ ë•ŒëŠ” ê·¸ëƒ¥ ì˜¤íŠ¸ë°€+ë”°ëœ»í•œ ë¬¼+ì„¤íƒ• í•´ì„œ ë¨¹ì—ˆëŠ”ë° ì´ê²Œ ì •ì„ ì•„ë‹Œê°€'}\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["######################################################################\n","## prompt ##\n","nan\n","######################################################################\n","## chosen ##\n","ì¬í¬ë¦¼ì´ ì€ê·¼ ë³´í˜¸í•´ì£¼ë‚˜ ë´„ í‚¤í‚¤\n","######################################################################\n","## rejected ##\n","nan\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# configure optimizer\n","if args.strategy.startswith('colossalai'):\n","    optim = HybridAdam(model.parameters(), lr=5e-5)\n","else:\n","    optim = Adam(model.parameters(), lr=5e-5)"],"metadata":{"id":"tX0waMtjwO4n","executionInfo":{"status":"ok","timestamp":1715533763410,"user_tz":-540,"elapsed":416,"user":{"displayName":"ê¹€ì˜ˆë‘","userId":"06765726933479701190"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["# batch_size here is expected to be C(k,2), k means # response of each prompt\n","# be limited with the format of dataset 'Dahoas/rm-static', we'd better use batch_size as 1\n","trainer = RewardModelTrainer(model=model,\n","                             strategy=strategy,\n","                             optim=optim,\n","                             train_dataset=train_dataset,\n","                             eval_dataset=eval_dataset,\n","                             batch_size=args.batch_size,\n","                             max_epochs=args.max_epochs)"],"metadata":{"id":"gOHVg3axwPOa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train!!\n","trainer.fit(use_lora=args.lora_rank)\n","\n","## save\n","# save model checkpoint after fitting on only rank0\n","strategy.save_model(model, os.path.join(args.output_dir, 'RM.pt'), only_rank0=True)\n","# save optimizer checkpoint on all ranks\n","strategy.save_optimizer(optim,\n","                        os.path.join(args.output_dir, 'RM_optim_checkpoint_%d.pt' % (torch.cuda.current_device())),\n","                        only_rank0=False)\n","\n","model.save_pretrained(args.output_dir)  # config.json ìƒì„±"],"metadata":{"id":"E8AQCz0dwLL6"},"execution_count":null,"outputs":[]}]}