{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyP1boEqq11Gqbfmlawrb4kw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["\"\"\"\n","function ClickConnect(){\n","    console.log(\"코랩 연결 끊김 방지\");\n","    document.querySelector(\"colab-toolbar-button#connect\").click()\n","}\n","setInterval(ClickConnect, 60 * 1000)\n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"897oaVuom9IY","executionInfo":{"status":"ok","timestamp":1715530887396,"user_tz":-540,"elapsed":466,"user":{"displayName":"김예랑","userId":"06765726933479701190"}},"outputId":"2c2bc866-c376-4425-8001-f5b900ff24ca"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nfunction ClickConnect(){\\n    console.log(\"코랩 연결 끊김 방지\");\\n    document.querySelector(\"colab-toolbar-button#connect\").click()\\n}\\nsetInterval(ClickConnect, 60 * 1000)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["##전체 파일 합치기"],"metadata":{"id":"VHRGtW8u9_z9"}},{"cell_type":"code","source":["#----------------------------csv파일 합치기---------------------------------------\n","# import os\n","# import pandas as pd\n","\n","# directory = '/content/drive/MyDrive/2024_1/nlp/fuckingoriginal' #해당 디렉토리에 있는 모든 파일 하나로 합치기\n","# all_files = []\n","\n","# for filename in os.listdir(directory):\n","#     if filename.endswith('.csv'):\n","#         file_path = os.path.join(directory, filename) # 파일 경로 생성\n","#         all_files.append(pd.read_csv(file_path))\n","\n","# combined_df = pd.concat(all_files, ignore_index=True)\n","\n","# save_filename = 'original_conversation'\n","# combined_df.to_csv(f'/content/drive/MyDrive/{save_filename}.csv', index=False)\n","\n","#----------------------------JSON파일 합치기---------------------------------------\n","import os\n","import json\n","\n","directory = '/content/drive/MyDrive/2024_1/nlp/fuckingoriginal'\n","all_data = []\n","\n","for filename in os.listdir(directory):\n","    if filename.endswith('.json'):\n","        file_path = os.path.join(directory, filename) # 파일 경로 생성\n","        with open(file_path, 'r', encoding='utf-8') as file:\n","            data = json.load(file)\n","            all_data.extend(data)\n","\n","output_file_path = '/content/drive/MyDrive/original_conversation.json'\n","with open(output_file_path, 'w', encoding='utf-8') as file:\n","    json.dump(all_data, file, ensure_ascii=False, indent=4)\n"],"metadata":{"id":"CorbseUhX_m8","executionInfo":{"status":"ok","timestamp":1715530274985,"user_tz":-540,"elapsed":441,"user":{"displayName":"김예랑","userId":"06765726933479701190"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["##Load Packages##"],"metadata":{"id":"OXql23cBd-j-"}},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"gZ8o1LaDSj2F","executionInfo":{"status":"ok","timestamp":1715530500217,"user_tz":-540,"elapsed":224468,"user":{"displayName":"김예랑","userId":"06765726933479701190"}},"outputId":"0335b7b4-02e1-4e8c-8fbd-ad08660fb40b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: torch 1.13.1+cu116\n","Uninstalling torch-1.13.1+cu116:\n","  Successfully uninstalled torch-1.13.1+cu116\n","Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu116\n","Collecting torch==1.13.1+cu116\n","  Using cached https://download.pytorch.org/whl/cu116/torch-1.13.1%2Bcu116-cp310-cp310-linux_x86_64.whl (1977.9 MB)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1+cu116) (4.11.0)\n","Installing collected packages: torch\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 1.13.1+cu116 which is incompatible.\n","torchdata 0.7.1 requires torch>=2, but you have torch 1.13.1+cu116 which is incompatible.\n","torchtext 0.17.1 requires torch==2.2.1, but you have torch 1.13.1+cu116 which is incompatible.\n","torchvision 0.17.1+cu121 requires torch==2.2.1, but you have torch 1.13.1+cu116 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed torch-1.13.1+cu116\n","Torch version:1.13.1+cu116\n","cuda version: 11.6\n","cudnn version:8302\n","Requirement already satisfied: transformers==4.35.2 in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (3.14.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (0.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (0.15.2)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (4.66.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.2) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.2) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.2) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.2) (2024.2.2)\n","Requirement already satisfied: accelerate==0.24.1 in /usr/local/lib/python3.10/dist-packages (0.24.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.24.1) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.24.1) (23.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.24.1) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.24.1) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.24.1) (1.13.1+cu116)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.24.1) (0.23.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.24.1) (4.11.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.24.1) (3.14.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.24.1) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.24.1) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.24.1) (4.66.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.24.1) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.24.1) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.24.1) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.24.1) (2024.2.2)\n","Requirement already satisfied: colossalai==0.2.7 in /usr/local/lib/python3.10/dist-packages (0.2.7)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (1.25.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (4.66.4)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (5.9.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (23.2)\n","Requirement already satisfied: pre-commit in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (3.7.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (13.7.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (8.1.7)\n","Requirement already satisfied: fabric in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (3.2.2)\n","Requirement already satisfied: contexttimer in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (0.3.3)\n","Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (1.11.1.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (1.13.1+cu116)\n","Requirement already satisfied: invoke>=2.0 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai==0.2.7) (2.2.0)\n","Requirement already satisfied: paramiko>=2.4 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai==0.2.7) (3.4.0)\n","Requirement already satisfied: decorator>=5 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai==0.2.7) (5.1.1)\n","Requirement already satisfied: deprecated>=1.2 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai==0.2.7) (1.2.14)\n","Requirement already satisfied: cfgv>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai==0.2.7) (3.4.0)\n","Requirement already satisfied: identify>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai==0.2.7) (2.5.36)\n","Requirement already satisfied: nodeenv>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai==0.2.7) (1.8.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai==0.2.7) (6.0.1)\n","Requirement already satisfied: virtualenv>=20.10.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai==0.2.7) (20.26.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai==0.2.7) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai==0.2.7) (2.16.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->colossalai==0.2.7) (4.11.0)\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2->fabric->colossalai==0.2.7) (1.14.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->colossalai==0.2.7) (0.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nodeenv>=0.11.1->pre-commit->colossalai==0.2.7) (67.7.2)\n","Requirement already satisfied: bcrypt>=3.2 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai==0.2.7) (4.1.3)\n","Requirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai==0.2.7) (42.0.7)\n","Requirement already satisfied: pynacl>=1.5 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai==0.2.7) (1.5.0)\n","Requirement already satisfied: distlib<1,>=0.3.7 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai==0.2.7) (0.3.8)\n","Requirement already satisfied: filelock<4,>=3.12.2 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai==0.2.7) (3.14.0)\n","Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai==0.2.7) (4.2.1)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.3->paramiko>=2.4->fabric->colossalai==0.2.7) (1.16.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric->colossalai==0.2.7) (2.22)\n","fatal: destination path 'KoChatGPT' already exists and is not an empty directory.\n","mv: cannot stat 'KoChatGPT/data_kochatgpt': No such file or directory\n","mv: cannot stat 'KoChatGPT/img': No such file or directory\n","/content/KoChatGPT/colossalai_ChatGPT_230319\n","Processing /content/KoChatGPT/colossalai_ChatGPT_230319\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: transformers>=4.20.1 in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (4.35.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (4.66.4)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (2.19.1)\n","Requirement already satisfied: loralib in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (0.1.2)\n","Requirement already satisfied: colossalai>=0.2.4 in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (0.2.7)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (1.13.1+cu116)\n","Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (0.0.113)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (1.25.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (5.9.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (23.2)\n","Requirement already satisfied: pre-commit in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (3.7.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (13.7.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (8.1.7)\n","Requirement already satisfied: fabric in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (3.2.2)\n","Requirement already satisfied: contexttimer in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (0.3.3)\n","Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (1.11.1.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (3.14.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (0.23.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (0.15.2)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (0.4.3)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (0.6)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (2.0.3)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (0.70.16)\n","Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (3.9.5)\n","Requirement already satisfied: SQLAlchemy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain->chatgpt==0.1.0) (1.4.52)\n","Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain->chatgpt==0.1.0) (0.5.14)\n","Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain->chatgpt==0.1.0) (1.10.15)\n","Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain->chatgpt==0.1.0) (8.3.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->chatgpt==0.1.0) (4.11.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (4.0.3)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0) (3.21.2)\n","Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0) (0.9.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.20.1->chatgpt==0.1.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.20.1->chatgpt==0.1.0) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.20.1->chatgpt==0.1.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.20.1->chatgpt==0.1.0) (2024.2.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<2,>=1->langchain->chatgpt==0.1.0) (3.0.3)\n","Requirement already satisfied: invoke>=2.0 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai>=0.2.4->chatgpt==0.1.0) (2.2.0)\n","Requirement already satisfied: paramiko>=2.4 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai>=0.2.4->chatgpt==0.1.0) (3.4.0)\n","Requirement already satisfied: decorator>=5 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai>=0.2.4->chatgpt==0.1.0) (5.1.1)\n","Requirement already satisfied: deprecated>=1.2 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai>=0.2.4->chatgpt==0.1.0) (1.2.14)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->chatgpt==0.1.0) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->chatgpt==0.1.0) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->chatgpt==0.1.0) (2024.1)\n","Requirement already satisfied: cfgv>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (3.4.0)\n","Requirement already satisfied: identify>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (2.5.36)\n","Requirement already satisfied: nodeenv>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (1.8.0)\n","Requirement already satisfied: virtualenv>=20.10.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (20.26.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai>=0.2.4->chatgpt==0.1.0) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai>=0.2.4->chatgpt==0.1.0) (2.16.1)\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (1.14.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->colossalai>=0.2.4->chatgpt==0.1.0) (0.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nodeenv>=0.11.1->pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (67.7.2)\n","Requirement already satisfied: bcrypt>=3.2 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (4.1.3)\n","Requirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (42.0.7)\n","Requirement already satisfied: pynacl>=1.5 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (1.5.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->chatgpt==0.1.0) (1.16.0)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0) (1.0.0)\n","Requirement already satisfied: distlib<1,>=0.3.7 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (0.3.8)\n","Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (4.2.1)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.3->paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (1.16.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (2.22)\n","Building wheels for collected packages: chatgpt\n","  Building wheel for chatgpt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for chatgpt: filename=chatgpt-0.1.0-py3-none-any.whl size=46645 sha256=057922979e9c970a6d9e74d7839c4e42cc17b37a89af829f28d620f424707c08\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-kdkxqvrj/wheels/2b/0d/f8/b8f3ba4fd18f31a4096ee5bf83e4579cb54597f393eeae227c\n","Successfully built chatgpt\n","Installing collected packages: chatgpt\n","  Attempting uninstall: chatgpt\n","    Found existing installation: chatgpt 0.1.0\n","    Uninstalling chatgpt-0.1.0:\n","      Successfully uninstalled chatgpt-0.1.0\n","Successfully installed chatgpt-0.1.0\n","/content\n","Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.28.1)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.15)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n","Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n","Requirement already satisfied: langchain==0.0.113 in /usr/local/lib/python3.10/dist-packages (0.0.113)\n","Requirement already satisfied: PyYAML<7,>=6 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (6.0.1)\n","Requirement already satisfied: SQLAlchemy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (1.4.52)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (3.9.5)\n","Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (0.5.14)\n","Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (1.25.2)\n","Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (1.10.15)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (2.31.0)\n","Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (8.3.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (4.0.3)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (3.21.2)\n","Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (0.9.0)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain==0.0.113) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.113) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.113) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.113) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.113) (2024.2.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<2,>=1->langchain==0.0.113) (3.0.3)\n","Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (23.2)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (1.0.0)\n"]}],"source":["## setup(1min)\n","# torch 버전 다운. torch>=2.0 에선 colosalai가 동작안함\n","!pip uninstall torch -y\n","!pip install torch==1.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116\n","\n","import torch\n","\n","print(\"Torch version:{}\".format(torch.__version__))\n","print(\"cuda version: {}\".format(torch.version.cuda))\n","print(\"cudnn version:{}\".format(torch.backends.cudnn.version()))\n","\n","# for transformers, 최신버전은 에러발생\n","!pip install transformers==4.35.2\n","!pip install accelerate==0.24.1\n","\n","# for ColossalAI\n","!pip install colossalai==0.2.7\n","\n","# setup data\n","!git clone https://github.com/airobotlab/KoChatGPT\n","!mv KoChatGPT/data_kochatgpt .\n","!mv KoChatGPT/img .\n","\n","%cd KoChatGPT/colossalai_ChatGPT_230319/\n","!pip install .\n","%cd ../../\n","\n","# setup library\n","!pip install openai\n","!pip install langchain==0.0.113\n","!pip install pandas>=1.4.1"]},{"cell_type":"code","source":["# import\n","import os\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","from datasets import load_dataset\n","import transformers\n","from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, pipeline\n","from transformers import Trainer, TrainingArguments, AutoModelWithLMHead\n","from copy import deepcopy\n","from torch.optim import Adam\n","from transformers import AutoTokenizer, BloomTokenizerFast\n","from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n","import pandas as pd\n","import argparse\n","import copy\n","import logging\n","import json\n","from dataclasses import dataclass, field\n","\n","def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):\n","    \"\"\"Collects the state dict and dump to disk.\"\"\"\n","    state_dict = trainer.model.state_dict()\n","    if trainer.args.should_save:\n","        cpu_state_dict = {key: value.cpu() for key, value in list(state_dict.items())}\n","        del state_dict\n","        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa"],"metadata":{"id":"k4KFYan9SsQJ","executionInfo":{"status":"ok","timestamp":1715530528112,"user_tz":-540,"elapsed":27923,"user":{"displayName":"김예랑","userId":"06765726933479701190"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pAeFcKQXV7Rm","executionInfo":{"status":"ok","timestamp":1715530531233,"user_tz":-540,"elapsed":3128,"user":{"displayName":"김예랑","userId":"06765726933479701190"}},"outputId":"118a9e02-5bd7-4a6d-fd2e-52459e4ac10f"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["###(참고) 제공된 코드의 데이터 파일 형식"],"metadata":{"id":"_aIHzhXs-GMT"}},{"cell_type":"code","source":["import json\n","\n","with open('/content/data_kochatgpt/kochatgpt_1_SFT.jsonl','r') as f:\n","  ff = json.load(f)\n","print(ff[1])\n","print(ff[1]['prompt'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iin7DvETS3VV","executionInfo":{"status":"ok","timestamp":1715530531233,"user_tz":-540,"elapsed":7,"user":{"displayName":"김예랑","userId":"06765726933479701190"}},"outputId":"3599b27e-748f-401d-fba7-e27223816fb3"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["{'prompt': '쓰던 앱이 유료로 전환됐어', 'completion': \"'어떤 앱인지 모르기 때문에 정확한 답변을 드리기 어렵습니다. 하지만, 일반적으로 유료 전환된 앱은 기존 무료 앱에서 추가적인 기능이나 서비스를 제공하는 경우가 많습니다. 이 경우, 유료 전환 이전 가격이 매우 저렴하거나 무료인 경우가 많으므로, 앱을 계속 사용하려면 추가적인 비용을 지불해야 할 수 있습니다. 하지만, 유료 전환 이후에는 기존 사용자 또는 새로운 사용자 모두가 추가 기능과 높은 품질의 서비스를 이용할 수 있게 됩니다. 따라서, 앱 개발자는 유료 모델로 전환함으로써 앱의 수익을 증가시키고 더 나은 서비스를 제공할 수 있습니다.\", 'tokens': 288}\n","쓰던 앱이 유료로 전환됐어\n"]}]},{"cell_type":"markdown","source":["##Finetune SFT (우린 안해도 될듯)"],"metadata":{"id":"-Wy7JlkgeCFL"}},{"cell_type":"code","source":["# define argment\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--data_path_1_SFT', type=str, default='./content/drive/MyDrive/combined_final_file.csv')\n","parser.add_argument('--model_name', type=str, default='gpt2', choices=['gpt2', 'bloom', 'opt'])\n","parser.add_argument('--max_epochs', type=int, default=2)\n","parser.add_argument('--train_batch_size', type=int, default=8)\n","parser.add_argument('--output_dir', type=str, default='./output_1_SFT')\n","\n","args = parser.parse_args(args=[])\n","\n","# for test\n","args.model_name = 'skt/kogpt2-base-v2'  # SK GPT2, https://github.com/SKT-AI/KoGPT2\n","args.max_epochs = 2\n","\n","print(args)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KgOvAY68fBd7","executionInfo":{"status":"ok","timestamp":1715530837346,"user_tz":-540,"elapsed":441,"user":{"displayName":"김예랑","userId":"06765726933479701190"}},"outputId":"b7f6fbac-9df0-4259-be43-c9187c03dd44"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Namespace(data_path_1_SFT='./content/drive/MyDrive/combined_final_file.csv', model_name='skt/kogpt2-base-v2', max_epochs=2, train_batch_size=8, output_dir='./output_1_SFT')\n"]}]},{"cell_type":"code","source":["## test & load skt gpt2 kroean\n","import torch\n","from transformers import GPT2LMHeadModel, pipeline\n","\n","from transformers import PreTrainedTokenizerFast\n","tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n","                                                    bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n","                                                    pad_token='<pad>', mask_token='<mask>')\n","print(tokenizer.tokenize(\"안녕하세요. 한국어 GPT-2 입니다.😤:)l^o\"))\n","# ['▁안녕', '하', '세', '요.', '▁한국어', '▁G', 'P', 'T', '-2', '▁입', '니다.', '😤', ':)', 'l^o']\n","\n","\n","model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n","text = '근육이 커지기 위해서는'\n","input_ids = tokenizer.encode(text, return_tensors='pt')\n","gen_ids = model.generate(input_ids,\n","                         max_length=128,\n","                         repetition_penalty=2.0,\n","                         pad_token_id=tokenizer.pad_token_id,\n","                         eos_token_id=tokenizer.eos_token_id,\n","                         bos_token_id=tokenizer.bos_token_id,\n","                         use_cache=True)\n","generated = tokenizer.decode(gen_ids[0])\n","# print(generated)\n","\n","\n","generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n","generation_args = dict(\n","    num_beams=4,\n","    repetition_penalty=2.0,\n","    no_repeat_ngram_size=4,\n","    eos_token_id=375, # \\n\n","    max_new_tokens=64,\n","    do_sample=True,\n","    top_k=50,\n","    early_stopping=True\n",")\n","# generator(\n","#     [\"0 : **는 게임 좋아하니\\n1 :\",\n","#     \"0 : 어제 강남에서 살인사건 났대 ㅜㅜ 너무 무서워\\n1 : 헐 왜? 무슨 일 있었어?\\n0 : 사진보니까 막 피흘리는 사람있고 경찰들이 떠서 제압하고 난리도 아니었다던데??\\n1 :\",\n","#     \"0 : 자기야 어제는 나한테 왜 그랬어?\\n1 : 뭔 일 있었어?\\n0 : 어떻게 나한테 말도 없이 그럴 수 있어? 나 진짜 실망했어\\n1 : \"],\n","#     **generation_args\n","# )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"TGzmiumsS4Cx","executionInfo":{"status":"ok","timestamp":1715512109320,"user_tz":-540,"elapsed":35922,"user":{"displayName":"김예랑","userId":"06765726933479701190"}},"outputId":"28546631-f90a-4cf8-c5a3-142824bfe2d1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n","The class this function is called from is 'PreTrainedTokenizerFast'.\n"]},{"output_type":"stream","name":"stdout","text":["['▁안녕', '하', '세', '요.', '▁한국어', '▁G', 'P', 'T', '-2', '▁입', '니다.', '😤', ':)', 'l^o']\n","근육이 커지기 위해서는 무엇보다 규칙적인 생활습관이 중요하다.\n","특히, 아침식사는 단백질과 비타민이 풍부한 과일과 채소를 많이 섭취하는 것이 좋다.\n","또한 하루 30분 이상 충분한 수면을 취하는 것도 도움이 된다.\n","아침 식사를 거르지 않고 규칙적으로 운동을 하면 혈액순환에 도움을 줄 뿐만 아니라 신진대사를 촉진해 체내 노폐물을 배출하고 혈압을 낮춰준다.\n","운동은 하루에 10분 정도만 하는 게 좋으며 운동 후에는 반드시 스트레칭을 통해 근육량을 늘리고 유연성을 높여야 한다.\n","운동 후 바로 잠자리에 드는 것은 피해야 하며 특히 아침에 일어나면 몸이 피곤해지기 때문에 무리하게 움직이면 오히려 역효과가 날 수도 있다.\n","운동을\n"]},{"output_type":"execute_result","data":{"text/plain":["[[{'generated_text': '0 : **는 게임 좋아하니\\n1 : ***-****\\n'}],\n"," [{'generated_text': '0 : 어제 강남에서 살인사건 났대 ㅜㅜ 너무 무서워\\n1 : 헐 왜? 무슨 일 있었어?\\n0 : 사진보니까 막 피흘리는 사람있고 경찰들이 떠서 제압하고 난리도 아니었다던데??\\n1 : 아, 저게 진짜 아니야???\\n0 : 이거 어떻게 된 거야????\\n2 : 그건 뭐지?\\n3 : 그냥 찍었잖아..\\n3 : 이건 뭐야??!\\n3 : 어어?\\n3 : 우연찮게 생긴 여고생들 때문에'}],\n"," [{'generated_text': '0 : 자기야 어제는 나한테 왜 그랬어?\\n1 : 뭔 일 있었어?\\n0 : 어떻게 나한테 말도 없이 그럴 수 있어? 나 진짜 실망했어\\n1 : 뭘 그런 식으로 얘기하는 거야? 내가 화를 내면서 말했잖아.\\n2 : 뭐라고 했어?\\n3 : 무슨 일이 있었어?\\n4 : 너한테 거짓말을 한 거야?\\n5 : 아무 일도 없었어?\\n6 : 그건 네 잘못이야!\\n7 : 아냐, 이게'}]]"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["# data config\n","IGNORE_INDEX = -100\n","DEFAULT_PAD_TOKEN = \"[PAD]\"\n","DEFAULT_EOS_TOKEN = \"</s>\"\n","DEFAULT_BOS_TOKEN = \"</s>\"\n","DEFAULT_UNK_TOKEN = \"</s>\"\n","prompt = \"null\"\n","PROMPT_DICT = {\n","    \"prompt_input\": (\n","        \"욕설이 포함된 아래 입력을 욕설이 없는 예쁜 한국어 문장으로 바꿔줘. 아래 예시를 참고하고 입력 문장과 짧거나 비슷한 길이로 만들어줘\\n\\n\"\n","        \"요청을 적절히 완료하는 응답을 작성하세요.\\n\\n\"\n","        f\"### Instruction(명령어):\\n{prompt}\\n\\n### Input(입력):\\n{input}\\n\\n### Response(응답):\"\n","    ),\n","    \"prompt_no_input\": ( #<-이 버전으로 쓸듯\n","        \"욕설이 포함된 아래 입력을 욕설이 없는 예쁜 한국어 문장으로 바꿔줘. 아래 예시를 참고하고 입력 문장과 짧거나 비슷한 길이로 만들어줘\\n\\nn\"\n","        \"input : 우왕 너무 배고파~~\\n\\n\"\n","        \"output : 배고프노 이기야 ~~\\n\\n\"\n","        \"input : 과제 너무 많아서 하기가 싫어\\n\\n\"\n","        \"output : 라도출신 교수게이가 과제를 노무노무 많이내서 하기가 싫노~ .\\n\\n\"\n","        f\"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n","    ),\n","}"],"metadata":{"id":"dOpkqknObfb9","executionInfo":{"status":"ok","timestamp":1715530846306,"user_tz":-540,"elapsed":456,"user":{"displayName":"김예랑","userId":"06765726933479701190"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["model = AutoModelForCausalLM.from_pretrained(args.model_name)\n","tokenizer = transformers.AutoTokenizer.from_pretrained(\n","    args.model_name,\n","    padding_side=\"right\",\n","    model_max_length=512,\n",")\n","tokenizer.add_special_tokens(\n","    {\n","        \"eos_token\": DEFAULT_EOS_TOKEN,\n","        \"bos_token\": DEFAULT_BOS_TOKEN,\n","        \"unk_token\": DEFAULT_UNK_TOKEN,\n","    }\n",")\n","tokenizer.pad_token = tokenizer.eos_token\n","print(tokenizer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"Q5KWEpUieycL","executionInfo":{"status":"ok","timestamp":1715530860845,"user_tz":-540,"elapsed":13111,"user":{"displayName":"김예랑","userId":"06765726933479701190"}},"outputId":"307f94b1-8e8e-4fba-91b9-ca1d420a3606"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["GPT2TokenizerFast(name_or_path='skt/kogpt2-base-v2', vocab_size=51200, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t1: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t2: AddedToken(\"<usr>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t3: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t4: AddedToken(\"<sys>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t5: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t6: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t7: AddedToken(\"<d>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t8: AddedToken(\"</d>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t9: AddedToken(\"<unused0>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t10: AddedToken(\"<unused1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t11: AddedToken(\"<unused2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t12: AddedToken(\"<unused3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t13: AddedToken(\"<unused4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t14: AddedToken(\"<unused5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t15: AddedToken(\"<unused6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t16: AddedToken(\"<unused7>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t17: AddedToken(\"<unused8>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t18: AddedToken(\"<unused9>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t19: AddedToken(\"<unused10>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t20: AddedToken(\"<unused11>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t21: AddedToken(\"<unused12>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t22: AddedToken(\"<unused13>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t23: AddedToken(\"<unused14>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t24: AddedToken(\"<unused15>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t25: AddedToken(\"<unused16>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t26: AddedToken(\"<unused17>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t27: AddedToken(\"<unused18>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t28: AddedToken(\"<unused19>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t29: AddedToken(\"<unused20>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t30: AddedToken(\"<unused21>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t31: AddedToken(\"<unused22>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t32: AddedToken(\"<unused23>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t33: AddedToken(\"<unused24>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t34: AddedToken(\"<unused25>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t35: AddedToken(\"<unused26>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t36: AddedToken(\"<unused27>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t37: AddedToken(\"<unused28>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t38: AddedToken(\"<unused29>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t39: AddedToken(\"<unused30>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t40: AddedToken(\"<unused31>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t41: AddedToken(\"<unused32>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t42: AddedToken(\"<unused33>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t43: AddedToken(\"<unused34>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t44: AddedToken(\"<unused35>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t45: AddedToken(\"<unused36>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t46: AddedToken(\"<unused37>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t47: AddedToken(\"<unused38>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t48: AddedToken(\"<unused39>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t49: AddedToken(\"<unused40>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50: AddedToken(\"<unused41>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t51: AddedToken(\"<unused42>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t52: AddedToken(\"<unused43>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t53: AddedToken(\"<unused44>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t54: AddedToken(\"<unused45>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t55: AddedToken(\"<unused46>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t56: AddedToken(\"<unused47>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t57: AddedToken(\"<unused48>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t58: AddedToken(\"<unused49>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t59: AddedToken(\"<unused50>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t60: AddedToken(\"<unused51>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t61: AddedToken(\"<unused52>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t62: AddedToken(\"<unused53>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t63: AddedToken(\"<unused54>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t64: AddedToken(\"<unused55>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t65: AddedToken(\"<unused56>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t66: AddedToken(\"<unused57>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t67: AddedToken(\"<unused58>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t68: AddedToken(\"<unused59>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t69: AddedToken(\"<unused60>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t70: AddedToken(\"<unused61>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t71: AddedToken(\"<unused62>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t72: AddedToken(\"<unused63>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t73: AddedToken(\"<unused64>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t74: AddedToken(\"<unused65>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t75: AddedToken(\"<unused66>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t76: AddedToken(\"<unused67>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t77: AddedToken(\"<unused68>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t78: AddedToken(\"<unused69>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t79: AddedToken(\"<unused70>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t80: AddedToken(\"<unused71>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t81: AddedToken(\"<unused72>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t82: AddedToken(\"<unused73>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t83: AddedToken(\"<unused74>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t84: AddedToken(\"<unused75>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t85: AddedToken(\"<unused76>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t86: AddedToken(\"<unused77>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t87: AddedToken(\"<unused78>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t88: AddedToken(\"<unused79>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t89: AddedToken(\"<unused80>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t90: AddedToken(\"<unused81>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t91: AddedToken(\"<unused82>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t92: AddedToken(\"<unused83>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t93: AddedToken(\"<unused84>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t94: AddedToken(\"<unused85>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t95: AddedToken(\"<unused86>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t96: AddedToken(\"<unused87>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t97: AddedToken(\"<unused88>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t98: AddedToken(\"<unused89>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t99: AddedToken(\"<unused90>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t100: AddedToken(\"<unused91>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t101: AddedToken(\"<unused92>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t102: AddedToken(\"<unused93>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t103: AddedToken(\"<unused94>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t104: AddedToken(\"<unused95>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t105: AddedToken(\"<unused96>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t106: AddedToken(\"<unused97>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t107: AddedToken(\"<unused98>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t108: AddedToken(\"<unused99>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t109: AddedToken(\":-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t110: AddedToken(\":)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t111: AddedToken(\"-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t112: AddedToken(\"(-:\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t113: AddedToken(\"(:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t114: AddedToken(\"(:-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t115: AddedToken(\"-}\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t116: AddedToken(\"8-O\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t117: AddedToken(\"'-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t118: AddedToken(\":-#\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t119: AddedToken(\":-*\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t120: AddedToken(\":-/\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t121: AddedToken(\":->\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t122: AddedToken(\":-@\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t123: AddedToken(\":-d\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t124: AddedToken(\":-V\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t125: AddedToken(\":-X\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t126: AddedToken(\":-\\\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t127: AddedToken(\":-]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t128: AddedToken(\";-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t129: AddedToken(\">;->\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t130: AddedToken(\";^)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t131: AddedToken(\"%-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t132: AddedToken(\"):-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t133: AddedToken(\"3:]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t134: AddedToken(\":-&\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t135: AddedToken(\"8:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t136: AddedToken(\":-)8<\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t137: AddedToken(\":-O\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t138: AddedToken(\":-6\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t139: AddedToken(\"+:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t140: AddedToken(\"O:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t141: AddedToken(\":-<\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t142: AddedToken(\":-?\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t143: AddedToken(\":-E\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t144: AddedToken(\":-Q\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t145: AddedToken(\":-}X\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t146: AddedToken(\":-[\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t147: AddedToken(\":-a\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t148: AddedToken(\":-{\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t149: AddedToken(\":-{}\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t150: AddedToken(\":^)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t151: AddedToken(\"<:-l\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t152: AddedToken(\":=)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t153: AddedToken(\">:->\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t154: AddedToken(\">:-l\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t155: AddedToken(\"@:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t156: AddedToken(\"@:-}\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t157: AddedToken(\"C=:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t158: AddedToken(\"X:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t159: AddedToken(\"[:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t160: AddedToken(\"[:]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t161: AddedToken(\"{:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t162: AddedToken(\"l^o\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t163: AddedToken(\"}:^#)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t164: AddedToken(\":-(=)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t165: AddedToken(\"O-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t166: AddedToken(\":-3\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t167: AddedToken(\":=\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t168: AddedToken(\":-\"\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t169: AddedToken(\"P-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t170: AddedToken(\"?-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t171: AddedToken(\"d:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t172: AddedToken(\":8)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t173: AddedToken(\":-7\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t174: AddedToken(\"):-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t175: AddedToken(\":/\\)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t176: AddedToken(\"8(:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t177: AddedToken(\"([(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t178: AddedToken(\":-(*)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t179: AddedToken(\"&-l\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t180: AddedToken(\":-e\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t181: AddedToken(\":(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t182: AddedToken(\":,(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t183: AddedToken(\":-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t184: AddedToken(\":-P\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t185: AddedToken(\":-S\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t186: AddedToken(\":-C\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t187: AddedToken(\":-r\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t188: AddedToken(\":-t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t189: AddedToken(\":-W\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t190: AddedToken(\"X-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t191: AddedToken(\"l-O\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t192: AddedToken(\"l:-O\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t193: AddedToken(\"$-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t194: AddedToken(\":-!\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t195: AddedToken(\":----}\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t196: AddedToken(\"=:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t197: AddedToken(\"=:-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t198: AddedToken(\"3:[\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t199: AddedToken(\"8<:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t200: AddedToken(\":#)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t201: AddedToken(\"8-#\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t202: AddedToken(\"B-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t203: AddedToken(\"8-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t204: AddedToken(\"|-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t205: AddedToken(\"H-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t206: AddedToken(\"]-I\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t207: AddedToken(\"V^J\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t208: AddedToken(\"+-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t209: AddedToken(\"~:-P\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t210: AddedToken(\"`'\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t211: AddedToken(\"L-P\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t212: AddedToken(\"BI\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t213: AddedToken(\"O|\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t214: AddedToken(\"^^\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t215: AddedToken(\"ㅜㅜ\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t216: AddedToken(\"ㅠㅠ\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t217: AddedToken(\"ㅡㅡ\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t218: AddedToken(\"😠\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t219: AddedToken(\"👿\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t220: AddedToken(\"😧\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t221: AddedToken(\"😰\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t222: AddedToken(\"😲\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t223: AddedToken(\"😁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t224: AddedToken(\"🐻\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t225: AddedToken(\"🐱\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t226: AddedToken(\"😹\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t227: AddedToken(\"😼\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t228: AddedToken(\"🤡\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t229: AddedToken(\"🥶\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t230: AddedToken(\"😖\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t231: AddedToken(\"😕\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t232: AddedToken(\"🐮\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t233: AddedToken(\"🤠\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t234: AddedToken(\"😿\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t235: AddedToken(\"😢\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t236: AddedToken(\"😞\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t237: AddedToken(\"😵\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t238: AddedToken(\"🐶\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t239: AddedToken(\"😓\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t240: AddedToken(\"🐲\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t241: AddedToken(\"🤤\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t242: AddedToken(\"😑\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t243: AddedToken(\"😘\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t244: AddedToken(\"😋\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t245: AddedToken(\"😱\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t246: AddedToken(\"🤮\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t247: AddedToken(\"🤭\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t248: AddedToken(\"🤕\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t249: AddedToken(\"😷\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t250: AddedToken(\"🧐\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t251: AddedToken(\"😮\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t252: AddedToken(\"🤨\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t253: AddedToken(\"🙄\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t254: AddedToken(\"😤\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t255: AddedToken(\"🤬\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t256: AddedToken(\"😂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t257: AddedToken(\"🤒\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t258: AddedToken(\"😛\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t259: AddedToken(\"😶\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t260: AddedToken(\"😨\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t261: AddedToken(\"🌛\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t262: AddedToken(\"😳\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t263: AddedToken(\"🦊\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t264: AddedToken(\"🐸\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t265: AddedToken(\"☹\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t266: AddedToken(\"☹️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t267: AddedToken(\"😦\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t268: AddedToken(\"🌝\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t269: AddedToken(\"😬\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t270: AddedToken(\"😺\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t271: AddedToken(\"😸\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t272: AddedToken(\"😀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t273: AddedToken(\"😃\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t274: AddedToken(\"😄\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t275: AddedToken(\"😅\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t276: AddedToken(\"😆\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t277: AddedToken(\"🐹\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t278: AddedToken(\"🐴\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t279: AddedToken(\"🥵\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t280: AddedToken(\"🤗\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t281: AddedToken(\"😯\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t282: AddedToken(\"😽\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t283: AddedToken(\"😗\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t284: AddedToken(\"😚\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t285: AddedToken(\"😙\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t286: AddedToken(\"🌜\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t287: AddedToken(\"🦁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t288: AddedToken(\"😭\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t289: AddedToken(\"🤥\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t290: AddedToken(\"🤦🏿‍♂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t291: AddedToken(\"🤦🏻‍♂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t292: AddedToken(\"🤦🏾‍♂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t293: AddedToken(\"🤦🏼‍♂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t294: AddedToken(\"🤦🏽‍♂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t295: AddedToken(\"🤦‍♂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t296: AddedToken(\"🤦🏿‍♂️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t297: AddedToken(\"🤦🏻‍♂️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t298: AddedToken(\"🤦🏾‍♂️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t299: AddedToken(\"🤦🏼‍♂️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t300: AddedToken(\"🤦🏽‍♂️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t301: AddedToken(\"🤦‍♂️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t302: AddedToken(\"🤑\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t303: AddedToken(\"🐵\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t304: AddedToken(\"🐭\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t305: AddedToken(\"🤢\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t306: AddedToken(\"🤓\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t307: AddedToken(\"😐\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t308: AddedToken(\"🌚\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t309: AddedToken(\"🐼\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t310: AddedToken(\"🥳\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t311: AddedToken(\"😔\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t312: AddedToken(\"😣\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t313: AddedToken(\"🤦\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t314: AddedToken(\"🤦🏿\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t315: AddedToken(\"🤦🏻\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t316: AddedToken(\"🤦🏾\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t317: AddedToken(\"🤦🏼\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t318: AddedToken(\"🤦🏽\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t319: AddedToken(\"🐷\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t320: AddedToken(\"🥺\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t321: AddedToken(\"😾\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t322: AddedToken(\"😡\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t323: AddedToken(\"🐰\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t324: AddedToken(\"😌\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t325: AddedToken(\"🤖\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t326: AddedToken(\"😥\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t327: AddedToken(\"🤫\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t328: AddedToken(\"😴\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t329: AddedToken(\"😪\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t330: AddedToken(\"🙁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t331: AddedToken(\"🙂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t332: AddedToken(\"😻\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t333: AddedToken(\"☺\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t334: AddedToken(\"☺️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t335: AddedToken(\"🥰\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t336: AddedToken(\"😇\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t337: AddedToken(\"😍\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t338: AddedToken(\"😈\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t339: AddedToken(\"😊\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t340: AddedToken(\"😎\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t341: AddedToken(\"😏\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t342: AddedToken(\"🤧\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t343: AddedToken(\"😝\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t344: AddedToken(\"🌞\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t345: AddedToken(\"🤔\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t346: AddedToken(\"🐯\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t347: AddedToken(\"😫\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t348: AddedToken(\"😒\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t349: AddedToken(\"🦄\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t350: AddedToken(\"🙃\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t351: AddedToken(\"🙀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t352: AddedToken(\"😩\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t353: AddedToken(\"🌬\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t354: AddedToken(\"🌬️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t355: AddedToken(\"😉\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t356: AddedToken(\"😜\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t357: AddedToken(\"🐺\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t358: AddedToken(\"🤦🏿‍♀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t359: AddedToken(\"🤦🏻‍♀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t360: AddedToken(\"🤦🏾‍♀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t361: AddedToken(\"🤦🏼‍♀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t362: AddedToken(\"🤦🏽‍♀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t363: AddedToken(\"🤦‍♀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t364: AddedToken(\"🤦🏿‍♀️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t365: AddedToken(\"🤦🏻‍♀️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t366: AddedToken(\"🤦🏾‍♀️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t367: AddedToken(\"🤦🏼‍♀️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t368: AddedToken(\"🤦🏽‍♀️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t369: AddedToken(\"🤦‍♀️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t370: AddedToken(\"🥴\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t371: AddedToken(\"😟\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t372: AddedToken(\"🥱\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t373: AddedToken(\"🤪\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t374: AddedToken(\"🤐\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t51200: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}\n"]}]},{"cell_type":"markdown","source":["###SFT를 위한 데이터셋 준비"],"metadata":{"id":"-b1b2UiafjfY"}},{"cell_type":"code","source":["from typing import Optional, Dict, Sequence\n","\n","class SFT_dataset(Dataset):\n","    '''SFT dataset by wygo'''\n","    def __init__(self, data_path_1_SFT: str, tokenizer: transformers.PreTrainedTokenizer, verbose=False):\n","        super(SFT_dataset, self).__init__()\n","        logging.warning(\"Loading data...\")\n","\n","        ## format\n","        pattern_instruction = 'prompt'  # 순화를 지시하는 명령 (프롬프트)\n","        pattern_input = 'input'  # 순화시킬 데이터\n","        pattern_output = 'completion'  # 순화된 output\n","\n","        ############################################################\n","        ## load dataset\n","        # 내 데이터셋엔 input이 없다 >> KoMo팀은 아니지롱 있지롱\n","        data_path_1_SFT = '/content/drive/MyDrive/combined_final_file.csv'\n","        list_data_csv = pd.read_csv(data_path_1_SFT, encoding='utf-8')\n","        profanity_data = list_data_csv['Profanity']\n","        cleaned_data = list_data_csv['Cleaned']\n","        if True:\n","          print(profanity_data[0])\n","        # with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n","        #     list_data_dict = json.load(json_file)\n","        #     if True:\n","        #       print(list_data_dict)\n","\n","        ############################################################\n","        ## 데이터셋 만들기, source와 target\n","        prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]  # 템플릿 가져오기\n","\n","        # 입력\n","        sources = []\n","        for example in profanity_data:\n","            if example != \"\":\n","                tmp = prompt_no_input.format(example) if isinstance(example, str) else prompt_no_input\n","            else:\n","                tmp = prompt_input\n","            sources.append(tmp)\n","\n","        # 출력\n","        targets = []\n","        for example in cleaned_data:\n","            targets.append(f\"{example}{tokenizer.eos_token}\" if isinstance(example, str) else f\"{tokenizer.eos_token}\")\n","\n","        if verbose:\n","            idx = 0\n","            print(sources[idx])\n","            print(targets[idx])\n","            print(\"Tokenizing inputs... This may take some time...\")\n","\n","        ############################################################\n","        # data_dict = preprocess(sources, targets, tokenizer)  # https://github.com/Beomi/KoAlpaca/blob/04704348d58b8b1c2e2638d6437a04b4e8ba1823/train.py#L124\n","        examples = [s + t for s, t in zip(sources, targets)]\n","\n","        # source data tokenized\n","        sources_tokenized = self._tokenize_fn(sources, tokenizer)  # source만\n","        examples_tokenized = self._tokenize_fn(examples, tokenizer)  # source + target\n","\n","\n","        ## 입력은 source, 출력은 source+target 이지만 학습은 target 부분만\n","        input_ids = examples_tokenized[\"input_ids\"]\n","        labels = copy.deepcopy(input_ids)\n","        for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n","            label[:source_len] = IGNORE_INDEX  # source 부분은 -100으로 채운다\n","\n","        data_dict = dict(input_ids=input_ids, labels=labels)\n","\n","        self.input_ids = data_dict[\"input_ids\"]\n","        self.labels = data_dict[\"labels\"]\n","        logging.warning(\"Loading data done!!: %d\"%(len(self.labels)))\n","\n","    def _tokenize_fn(self, strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n","        \"\"\"Tokenize a list of strings.\"\"\"\n","        tokenized_list = [\n","            tokenizer(\n","                text,\n","                return_tensors=\"pt\",\n","                padding=\"longest\",\n","                max_length=tokenizer.model_max_length,\n","                truncation=True,\n","            )\n","            for text in strings\n","        ]\n","        input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n","        input_ids_lens = labels_lens = [\n","            tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n","        ]\n","        return dict(\n","            input_ids=input_ids,\n","            labels=labels,\n","            input_ids_lens=input_ids_lens,\n","            labels_lens=labels_lens,\n","        )\n","\n","\n","    def __len__(self):\n","        return len(self.input_ids)\n","\n","\n","    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n","        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n","\n","\n","@dataclass\n","class DataCollatorForSupervisedDataset(object):\n","    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n","\n","    tokenizer: transformers.PreTrainedTokenizer\n","\n","    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n","        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n","        input_ids = torch.nn.utils.rnn.pad_sequence(\n","            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n","        )\n","        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n","        return dict(\n","            input_ids=input_ids,\n","            labels=labels,\n","            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n","        )\n","\n","\n","\n","train_dataset = SFT_dataset(data_path_1_SFT=args.data_path_1_SFT, tokenizer=tokenizer)\n","eval_dataset  = None  # eval은 안함\n","data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n","\n","# check\n","print('input : %s'%train_dataset.input_ids[0])\n","print('output: %s'%train_dataset.labels[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3GoVsw9ZezsV","executionInfo":{"status":"ok","timestamp":1715530883168,"user_tz":-540,"elapsed":22326,"user":{"displayName":"김예랑","userId":"06765726933479701190"}},"outputId":"b6865eab-7549-4790-a9a5-a1399faa655a"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:root:Loading data...\n"]},{"output_type":"stream","name":"stdout","text":["그냥 게임이나 하자고? ㅋㅋ 니가 게임하는거 보면 암걸릴거같아서 차라리 니 혼자 게임해라 ㅅㅂ 니랑 같이 하기 싫으니까 꺼져\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:Loading data done!!: 23533\n"]},{"output_type":"stream","name":"stdout","text":["input : tensor([12160, 16122, 20600,  9793,  9241,  9492, 12160, 16122,  9712, 41775,\n","        34407, 43257, 27729,  8244,   389,  9793,  9182, 11909, 11522,  9038,\n","        27318, 31618, 10669,  9314, 11451, 25243,  9659,  8244,   375,   375,\n","          452,  9610,   454, 14197,  9223, 40972, 12371,  9208,  6889,  8615,\n","          468,   468,   375,   375, 44682,   454, 14197,  9223,  9208,  6889,\n","         8688,  7119, 13779,  7991, 14380,   468,   375,   375,  9610,   454,\n","        14197,  9223, 28257, 12371, 27080,  9078,  9495, 16238,   375,   375,\n","        44682,   454, 14197,  9223,  9755,  7235, 27457, 13643,  6866,  9760,\n","        37932, 29777,  7119,  7556,  9564,  7071,  7788,  9078,  9495, 15254,\n","         7119,   468,   739,  9585,   375,   378,   378,   378, 14659, 13394,\n","        37091, 10651,   383, 25841,  8006, 14914,   375,   452, 36420,   375,\n","          375,   378,   378,   378, 41951,   454,  9549, 20549,   383,  8142,\n","         7192, 14914,  6947,  7084, 15403,  9185, 17709,   389, 14927,  9779,\n","        17537,  6958, 15254, 13083,     1])\n","output: tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  6947,  7084, 15403,  9185, 17709,   389, 14927,  9779,\n","        17537,  6958, 15254, 13083,     1])\n"]}]},{"cell_type":"code","source":["## 학습 (10min)\n","# training_args 수정 가능: https://github.com/Beomi/KoAlpaca/blob/main/train.sh 참고\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(device)\n","training_args = TrainingArguments(\n","    output_dir=\"./sft_test\", #The output directory\n","    overwrite_output_dir=True, #overwrite the content of the output directory\n","    num_train_epochs=1, # number of training epochs\n","    per_device_train_batch_size=4, # batch size for training\n","    per_device_eval_batch_size=4,  # batch size for evaluation\n","    eval_steps = 3, # Number of update steps between two evaluations.\n","    save_steps=500, # after # steps model is saved\n","    warmup_steps=5,# number of warmup steps for learning rate scheduler\n","    prediction_loss_only=True,\n","    )\n","trainer = Trainer(\n","    model=model.to(device),\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n",")\n","\n","trainer.train()\n","trainer.save_state()\n","safe_save_model_for_hf_trainer(trainer=trainer, output_dir=args.output_dir)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":450},"id":"fF3GJA6dgJsP","executionInfo":{"status":"error","timestamp":1715520550307,"user_tz":-540,"elapsed":16573,"user":{"displayName":"김예랑","userId":"06765726933479701190"}},"outputId":"af96213f-18a2-4da5-c5ed-a79ce7dd7817","collapsed":true},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2' max='5884' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [   2/5884 : < :, Epoch 0.00/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-46-b81f828826db>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0msafe_save_model_for_hf_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1555\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1556\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1557\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1859\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1860\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m                 if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2724\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2725\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2727\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2746\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2747\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2748\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2749\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2750\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1094\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m         \u001b[0mlm_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["##Reward Model"],"metadata":{"id":"7r9rVR2T2uOG"}},{"cell_type":"code","source":["# import\n","import argparse\n","\n","import loralib as lora\n","import torch\n","torch.cuda.empty_cache()\n","from chatgpt.dataset import RewardDataset\n","from chatgpt.models.base import RewardModel\n","from chatgpt.models.bloom import BLOOMRM\n","from chatgpt.models.gpt import GPTRM\n","from chatgpt.models.opt import OPTRM\n","from chatgpt.trainer import RewardModelTrainer\n","from chatgpt.trainer.strategies import ColossalAIStrategy, DDPStrategy, NaiveStrategy\n","from datasets import load_dataset\n","from torch.optim import Adam\n","from transformers import AutoTokenizer, BloomTokenizerFast\n","from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n","\n","from colossalai.nn.optimizer import HybridAdam\n","\n","import os\n","import json\n","\n","# data config\n","IGNORE_INDEX = -100\n","DEFAULT_PAD_TOKEN = \"[PAD]\"\n","DEFAULT_EOS_TOKEN = \"</s>\"\n","DEFAULT_BOS_TOKEN = \"</s>\"\n","\n","# define argment\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--output_dir', type=str, default='./output_2_RM')\n","parser.add_argument('--data_path_2_RM', type=str, default='./data_kochatgpt/kochatgpt_2_RM.jsonl', help='https://huggingface.co/datasets/fka/awesome-chatgpt-prompts/blob/main/prompts.csv')\n","parser.add_argument('--strategy',\n","                    choices=['naive', 'ddp', 'colossalai_gemini', 'colossalai_zero2'],\n","                    default='naive')\n","parser.add_argument('--model', type=str, default='gpt2', choices=['gpt2', 'bloom', 'opt'])\n","parser.add_argument('--pretrain', type=str, default=None)\n","parser.add_argument('--dataset', type=str, default='Dahoas/rm-static')\n","parser.add_argument('--save_path', type=str, default='rm_ckpt.pth')\n","parser.add_argument('--max_epochs', type=int, default=10)\n","parser.add_argument('--batch_size', type=int, default=4)\n","parser.add_argument('--lora_rank', type=int, default=0, help=\"low-rank adaptation matrices rank\")\n","parser.add_argument('--max_len', type=int, default=512)  # wygo 추가\n","\n","args = parser.parse_args(args=[])\n","\n","# for test\n","args.max_epochs = 3\n","args.pretrain = 'skt/kogpt2-base-v2'  # pretrained 모델 가져오기\n","args.verbose = True\n","\n","print(args)\n","if not os.path.exists(args.output_dir):\n","    os.makedirs(args.output_dir)\n","\n","# configure strategy\n","if args.strategy == 'naive':\n","    strategy = NaiveStrategy()\n","elif args.strategy == 'ddp':\n","    strategy = DDPStrategy()\n","elif args.strategy == 'colossalai_gemini':\n","    strategy = ColossalAIStrategy(stage=3, placement_policy='cuda')\n","elif args.strategy == 'colossalai_zero2':\n","    strategy = ColossalAIStrategy(stage=2, placement_policy='cuda')\n","else:\n","    raise ValueError(f'Unsupported strategy \"{args.strategy}\"')"],"metadata":{"id":"VxNrhTjz0MhO","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1715530908450,"user_tz":-540,"elapsed":1415,"user":{"displayName":"김예랑","userId":"06765726933479701190"}},"outputId":"aa4bd21e-1582-4817-b42b-6516db55b2aa"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/library.py:130: UserWarning: Overriding a previously registered kernel for the same operator and the same dispatch key\n","  operator: aten::index.Tensor(Tensor self, Tensor?[] indices) -> Tensor\n","    registered at aten/src/ATen/RegisterSchema.cpp:6\n","  dispatch key: Meta\n","  previous kernel: registered at ../aten/src/ATen/functorch/BatchRulesScatterOps.cpp:1053\n","       new kernel: registered at /dev/null:241 (Triggered internally at ../aten/src/ATen/core/dispatch/OperatorEntry.cpp:150.)\n","  self.m.impl(name, dispatch_key, fn)\n"]},{"output_type":"stream","name":"stdout","text":["Namespace(output_dir='./output_2_RM', data_path_2_RM='./data_kochatgpt/kochatgpt_2_RM.jsonl', strategy='naive', model='gpt2', pretrain='skt/kogpt2-base-v2', dataset='Dahoas/rm-static', save_path='rm_ckpt.pth', max_epochs=3, batch_size=4, lora_rank=0, max_len=512, verbose=True)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:202: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n","  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"]}]},{"cell_type":"code","source":["# customizing, https://github.com/hpcaitech/ColossalAI/blob/2e16f842a9e5b1fb54e7e41070e9d2bb5cd64d7c/applications/ChatGPT/chatgpt/nn/gpt_rm.py#L29\n","from typing import Optional\n","\n","import torch.nn as nn\n","from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n","from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n","\n","# from ..base import RewardModel\n","from chatgpt.models.base import RewardModel\n","\n","\n","class GPTRM_custom(RewardModel):\n","    \"\"\"\n","    GPT Reward model.\n","    Args:\n","        pretrained (str): Pretrained model name or path.\n","        config (GPT2Config): Model config.\n","        checkpoint (bool): Enable gradient checkpointing.\n","        lora_rank (int): Rank of the low-rank approximation.\n","        lora_train_bias (str): LoRA bias training mode.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 pretrained: Optional[str] = None,\n","                 config: Optional[GPT2Config] = None,\n","                 checkpoint: bool = False,\n","                 lora_rank: int = 0,\n","                 lora_train_bias: str = 'none',\n","                 tokenizer=None) -> None:\n","        if pretrained is not None:\n","            model = GPT2Model.from_pretrained(pretrained)\n","            model.resize_token_embeddings(len(tokenizer))  # wygo 추가!!!\n","        elif config is not None:\n","            model = GPT2Model(config)\n","        else:\n","            model = GPT2Model(GPT2Config())\n","        if checkpoint:\n","            model.gradient_checkpointing_enable()\n","\n","\n","        # model = model.resize_token_embeddings(len(tokenizer))\n","\n","        value_head = nn.Linear(model.config.n_embd, 1)\n","        super().__init__(model, value_head, lora_rank, lora_train_bias)\n","\n","        # 추가, 230421\n","        if pretrained is not None:\n","            self.model = model\n","            self.pretrained = pretrained\n","\n","    # 추가, 230421, config.json을 생성하기 위해 추가\n","    def save_pretrained(self, dir):\n","        if self.pretrained is not None:\n","            self.model.save_pretrained(dir)"],"metadata":{"id":"k0I7Udr4wgd1","executionInfo":{"status":"ok","timestamp":1715533412723,"user_tz":-540,"elapsed":494,"user":{"displayName":"김예랑","userId":"06765726933479701190"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["# configure model, tokenizer\n","with strategy.model_init_context():\n","    # load pretrained gpt2\n","    if args.model == 'gpt2':\n","#         tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","        # tokenizer = AutoTokenizer.from_pretrained(args.pretrain)\n","        tokenizer = AutoTokenizer.from_pretrained(args.pretrain, padding_side=\"right\", model_max_length=512)\n","        tokenizer.add_special_tokens(\n","            {\n","                \"eos_token\": DEFAULT_EOS_TOKEN,\n","                \"bos_token\": DEFAULT_BOS_TOKEN,\n","                \"unk_token\": DEFAULT_UNK_TOKEN,\n","            }\n","        )\n","        tokenizer.pad_token = tokenizer.eos_token\n","        model = GPTRM_custom(pretrained=args.pretrain, lora_rank=args.lora_rank, tokenizer=tokenizer) #.cuda() 붙여서 실행하기\n","\n","    elif args.model == 'bloom':\n","        model = BLOOMRM(pretrained=args.pretrain, lora_rank=args.lora_rank).cuda()\n","        tokenizer = BloomTokenizerFast.from_pretrained(args.pretrain)\n","\n","    elif args.model == 'opt':\n","        model = OPTRM(pretrained=args.pretrain, lora_rank=args.lora_rank).cuda()\n","        tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n","\n","    else:\n","        raise ValueError(f'Unsupported model \"{args.model}\"')\n","\n","\n","    # model.resize_token_embeddings(len(tokenizer))"],"metadata":{"id":"9qe8mVJ9wmpo","executionInfo":{"status":"ok","timestamp":1715533448048,"user_tz":-540,"elapsed":9973,"user":{"displayName":"김예랑","userId":"06765726933479701190"}}},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":["##Reward Model 데이터 형식\n","\n","{'prompt': '메뉴판 있어요?',\n","'completion_0': '죄송합니다, 제가 가상의 챗봇입니다. 제가 메뉴판을 제공할 수는 없습니다. 하지만, 혹시 궁금한 음식이나 음료가 있다면 알려주시면 그에 맞는 정보를 제공해드릴 수 있습니다.',\n","\n","'completion_1': '이락으로 잘알면 강의할 지 등 공개한 경개는 높아 가고 한다면 객체개를 위한 개서를 한다가 국토법인 경개를 높게 속한 경개는 일 하나 국토법인 경개를 들여 객체개는 더 국토법인 경개를 높이 한다면 객체개를 토법인 경개를 높이 한다고 국토법인 경개를 한다면 국토법인 경개를 높이 토법인 경개는 국토법인 경개를 높이 객체개를 토법인 경개를 한다면 국토법인 경개는다\\n\\n국토법인 경개를 들여 국토법인 경개를 국토법인 경개를 높이 국토법인 경개를 한다면 국토법인 경개를 한다면 국토법인 토법인 경개를 높이 한다면 국토법인 경개를 높이 한다면 국토법인 경개를 한다면 국토법인 경개를 높이 한다면 국토법인 경개를 한다면 국토법인 경개를 한다',\n","\n","'completion_2': '네, 여러가지 메뉴가 있습니다. 치킨, 피자, 샐러드, 스파게티, 그리고 다양한 간식과 음료 등이 있습니다.',\n","\n","'ranking': [0, 2, 1]}"],"metadata":{"id":"r-MwyeUSOnGY"}},{"cell_type":"markdown","source":["##Transfer KoMo data to RM data format"],"metadata":{"id":"KiS3B_LXPpoP"}},{"cell_type":"code","source":["import random\n","\n","rank0_path = '/content/drive/MyDrive/combined_emojis_final.csv' #원래문장에 이모지 있는,가장 랭크 높은 데이터\n","rank1_path = '/content/drive/MyDrive/original_conversation.json' #이모지 없고 문장만 착한 문장인 중간 랭크 데이터\n","rank2_path = '/content/drive/MyDrive/combined_final_file.csv'#욕이 남발하는 꼴등 데이터\n","\n","def read_json(path):\n","  with open(path,'r',encoding='utf-8') as f:\n","    data = json.load(f)\n","  return data\n","\n","def readcsv(path):\n","  data = pd.read_csv(path)\n","  return data\n","\n","def add_ranking_column(data, rank):\n","    data_with_rank = []\n","    for index, row in enumerate(data):\n","        data_dict = {\n","            'prompt': row,\n","            'ranking': rank\n","        }\n","        data_with_rank.append(data_dict)\n","    return data_with_rank\n","\n","rank0_data = readcsv(rank0_path)['Cleaned']\n","rank1_data = readcsv(rank2_path)['Cleaned'] #일반문장은 욕설 순화 전 문장과 같기 때문에 rank2의 'cleaned'로 처리\n","rank2_data = readcsv(rank2_path)['Profanity'] #욕설 데이터의 'profanity' 열 값\n","\n","rank0_data_with_rank = add_ranking_column(rank0_data, 0)\n","rank1_data_with_rank = add_ranking_column(rank1_data, 1)\n","rank2_data_with_rank = add_ranking_column(rank2_data, 2)\n","\n","list_data_dict = []\n","for i,(r0,r1,r2) in enumerate(zip(rank0_data_with_rank,rank1_data_with_rank,rank2_data_with_rank)):\n","  combined_lists = [r0,r1,r2]\n","  random.shuffle(combined_lists)\n","  temp_dict = {\n","      'prompt':r1['prompt'],\n","      'completion_0':combined_lists[0]['prompt'],\n","      'completion_1':combined_lists[1]['prompt'],\n","      'completion_2':combined_lists[2]['prompt'],\n","      'ranking':[combined_lists[0]['ranking'],combined_lists[1]['ranking'],combined_lists[2]['ranking']]\n","  }\n","  list_data_dict.append(temp_dict)"],"metadata":{"id":"sZv3hO4kPt7X","executionInfo":{"status":"ok","timestamp":1715532556883,"user_tz":-540,"elapsed":1704,"user":{"displayName":"김예랑","userId":"06765726933479701190"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["total_data_ranking2chosen = []\n","for tmp in list_data_dict:\n","    one_data_ranking2chosen = []\n","\n","    # data 1) 0 VS 1\n","    data = {}\n","    data['prompt'] = tmp['prompt']\n","    if tmp['ranking'][0] < tmp['ranking'][1]:\n","        data['chosen'] = str(tmp['completion_0'])\n","        data['rejected'] = tmp['completion_1']\n","    else:\n","        data['chosen'] = str(tmp['completion_1'])\n","        data['rejected'] = tmp['completion_0']\n","    one_data_ranking2chosen.append(data)\n","\n","\n","    # data 2) 0 VS 2\n","    data = {}\n","    data['prompt'] = tmp['prompt']\n","    if tmp['ranking'][0] < tmp['ranking'][2]:\n","        data['chosen'] = str(tmp['completion_0'])\n","        data['rejected'] = tmp['completion_2']\n","    else:\n","        data['chosen'] = str(tmp['completion_2'])\n","        data['rejected'] = tmp['completion_0']\n","    one_data_ranking2chosen.append(data)\n","\n","    # data 1) 1 VS 2\n","    data = {}\n","    data['prompt'] = tmp['prompt']\n","    if tmp['ranking'][1] < tmp['ranking'][2]:\n","        data['chosen'] = str(tmp['completion_1'])\n","        data['rejected'] = tmp['completion_2']\n","    else:\n","        data['chosen'] = str(tmp['completion_2'])\n","        data['rejected'] = tmp['completion_1']\n","    one_data_ranking2chosen.append(data)\n","\n","\n","\n","    total_data_ranking2chosen.extend(one_data_ranking2chosen)\n","\n","print('before data num: %d'%(len(list_data_dict)))\n","print('after  data num: %d'%(len(total_data_ranking2chosen)))\n","print('data example: \\n%s'%total_data_ranking2chosen[45])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0WfcoQe4_5p9","executionInfo":{"status":"ok","timestamp":1715533263188,"user_tz":-540,"elapsed":437,"user":{"displayName":"김예랑","userId":"06765726933479701190"}},"outputId":"59fa1e35-8639-4185-cc9d-fa0bee5482a1"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["before data num: 20835\n","after  data num: 62505\n","data example: \n","{'prompt': '겨우 2개? 한판도 못함 진짜', 'chosen': '겨우 2개? 한판도 못함 진짜', 'rejected': '겨우 2개 ㅋㅋㅋ 한판은 무슨 씹덕들이 2개로 오바하네 ㅉㅉ'}\n"]}]},{"cell_type":"code","source":["print(total_data_ranking2chosen[45])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dYqk0XD4t8HR","executionInfo":{"status":"ok","timestamp":1715533267278,"user_tz":-540,"elapsed":433,"user":{"displayName":"김예랑","userId":"06765726933479701190"}},"outputId":"91fd6ec1-caed-4b53-a978-aae675e9ea05"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["{'prompt': '겨우 2개? 한판도 못함 진짜', 'chosen': '겨우 2개? 한판도 못함 진짜', 'rejected': '겨우 2개 ㅋㅋㅋ 한판은 무슨 씹덕들이 2개로 오바하네 ㅉㅉ'}\n"]}]},{"cell_type":"code","source":["# prepare for data and dataset\n","import random\n","random.seed(230319)\n","# list_tmp = list(range(10))\n","random.shuffle(total_data_ranking2chosen)\n","print(total_data_ranking2chosen[45])\n","\n","# train_data = total_data_ranking2chosen[:-1000]  # 29000 학습\n","# eval_data = total_data_ranking2chosen[-1000:0]  # 1000개만 평가\n","\n","train_data = total_data_ranking2chosen[:100]  # 29000 학습\n","eval_data = total_data_ranking2chosen[100:130]  # 1000개만 평가\n","train_dataset = None\n","eval_dataset = None\n","for each in train_data:\n","  try:\n","    train_dataset = RewardDataset(each, tokenizer, args.max_len)\n","  except TypeError:\n","    continue\n","for each in eval_data:\n","  try:\n","    eval_dataset = RewardDataset(each, tokenizer, args.max_len)\n","  except TypeError:\n","      continue\n","\n","# check\n","# idx = 3\n","# print('#'*70)\n","# print('## prompt ##')\n","# print(train_data[idx]['prompt'])\n","# print('#'*70)\n","# print('## chosen ##')\n","# print(train_data[idx]['chosen'])\n","# print('#'*70)\n","# print('## rejected ##')\n","# print(train_data[idx]['rejected'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"qc7BzMF_t0p7","executionInfo":{"status":"ok","timestamp":1715533740982,"user_tz":-540,"elapsed":2092,"user":{"displayName":"김예랑","userId":"06765726933479701190"}},"outputId":"0e1dd127-21a5-41f0-f344-06a3d65302a8"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["{'prompt': '나 예전에 위장 안 좋을 때는 그냥 오트밀+따뜻한 물+설탕 해서 먹었는데 이게 정석 아닌가', 'chosen': '이제 낚시 갈 때 썩다배리 차 타고 가야겠어 🎣🚗', 'rejected': '나 예전에 위장 안 좋을 때는 그냥 오트밀+따뜻한 물+설탕 해서 먹었는데 이게 정석 아닌가'}\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]\n","  0%|          | 0/3 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["######################################################################\n","## prompt ##\n","nan\n","######################################################################\n","## chosen ##\n","썬크림이 은근 보호해주나 봄 키키\n","######################################################################\n","## rejected ##\n","nan\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# configure optimizer\n","if args.strategy.startswith('colossalai'):\n","    optim = HybridAdam(model.parameters(), lr=5e-5)\n","else:\n","    optim = Adam(model.parameters(), lr=5e-5)"],"metadata":{"id":"tX0waMtjwO4n","executionInfo":{"status":"ok","timestamp":1715533763410,"user_tz":-540,"elapsed":416,"user":{"displayName":"김예랑","userId":"06765726933479701190"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["# batch_size here is expected to be C(k,2), k means # response of each prompt\n","# be limited with the format of dataset 'Dahoas/rm-static', we'd better use batch_size as 1\n","trainer = RewardModelTrainer(model=model,\n","                             strategy=strategy,\n","                             optim=optim,\n","                             train_dataset=train_dataset,\n","                             eval_dataset=eval_dataset,\n","                             batch_size=args.batch_size,\n","                             max_epochs=args.max_epochs)"],"metadata":{"id":"gOHVg3axwPOa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train!!\n","trainer.fit(use_lora=args.lora_rank)\n","\n","## save\n","# save model checkpoint after fitting on only rank0\n","strategy.save_model(model, os.path.join(args.output_dir, 'RM.pt'), only_rank0=True)\n","# save optimizer checkpoint on all ranks\n","strategy.save_optimizer(optim,\n","                        os.path.join(args.output_dir, 'RM_optim_checkpoint_%d.pt' % (torch.cuda.current_device())),\n","                        only_rank0=False)\n","\n","model.save_pretrained(args.output_dir)  # config.json 생성"],"metadata":{"id":"E8AQCz0dwLL6"},"execution_count":null,"outputs":[]}]}